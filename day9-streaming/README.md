# 오늘의 서기: 윤지연
# {{day9-streaming}}

## 오늘의 핵심

- 1. 스트리밍은 실행 방식이 아니라 관측 방식이다

`graph.stream()` 및 `graph.astream()`은 그래프 실행을 스트리밍 형태로 관찰하기 위한 API이다.
실행 로직은 동일하며, `stream_mode`를 통해 **어떤 정보를 노출할 것인지**를 선택한다.

## stream_mode 개요

| mode       | 설명                   | 활용 예            |
| ---------- | -------------------- | --------------- |
| `updates`  | 각 스텝에서 변경된 상태만 스트리밍  | 상태 전이 추적, 로직 검증 |
| `values`   | 전체 상태를 계속 전달         | 상태 기반 UI, 리포트   |
| `messages` | LLM 토큰 단위 메시지 스트리밍   | 실시간 응답, 채팅 UI   |
| `custom`   | 노드 내부에서 정의한 임의 이벤트   | 진행률, 도메인 로그     |
| `debug`    | 실행 단계, 노드 정보 등 상세 로그 | 디버깅             |

> `stream_mode`는 출력 옵션이 아니라, **그래프 실행을 어떤 관점에서 관찰할지 결정하는 설정**이다.

---

- 2. messages + metadata를 이용한 이벤트 기반 필터링

`stream_mode="messages"`를 사용하면 `(message, metadata)` 형태의 스트림을 받을 수 있다.
이를 통해 토큰 단위 출력뿐 아니라 **출처 기반 필터링**이 가능하다.

## metadata로 구분 가능한 정보

* LLM 호출 구분 (`tags`)
* 그래프 노드 식별 (`langgraph_node`)
* 실행 단위(run, step) 정보

## 활용 시나리오

* 병렬 실행된 여러 LLM 중 특정 출력만 선택
* 노드 단위로 스트림 분리 처리
* 실시간 UI에서 목적별 출력 라우팅

> LangGraph의 메시지 스트리밍은 단순 토큰 출력이 아니라,
> **출처가 명확한 이벤트 스트림**을 제공한다.

---

- 3. LangGraph 스트리밍은 LLM 구현체에 종속되지 않는다

LangGraph의 스트리밍 구조는 특정 LLM 프레임워크(LangChain 등)에 의존하지 않는다.
핵심은 `get_stream_writer()`를 통한 **이벤트 방출 메커니즘**이다.

## 특징

* LangChain 미사용 LLM도 스트리밍에 통합 가능
* OpenAI SDK, 커스텀 모델, 외부 API 모두 동일한 방식으로 처리
* 비동기 토큰 생성기를 LangGraph 스트림으로 연결 가능


## 의미

* LangGraph는 토큰 생성 방식에 관여하지 않음
* 그래프 실행 중 발생하는 이벤트를 **표준화된 스트림으로 수집**하는 역할만 수행

> LangGraph는 LLM 호출 도구가 아니라,
> **이벤트 기반 실행 오케스트레이터**이다.

---

## 요약

* 스트리밍은 실행 결과 출력이 아니라 **실행 관측 방식**이다.
* `messages + metadata`를 통해 이벤트 단위 분기와 필터링이 가능하다.
* LangGraph 스트리밍은 LLM 구현과 분리된 독립적인 오케스트레이션 계층이다.

오늘의 예제는 LangGraph를 단순한 체인 실행 도구가 아니라,
**이벤트 중심 실행 엔진**으로 사용하는 방법을 보여준다.

- 
- 

## 회고 모음
- {{임정수}}: 비동기 처리는 한국인에게 잘 특화된 것 같습니다.
- {{양승준}}: 스트리밍의 개념과 사용법, 메세지스트림, 메타데이터 필터방법 그리고 서브그래프에 대해 배웠다
- {{박성빈}}: 이게 AI가 일하는 척 생색내는 방법이구나
- {{김동국}}: 뭔가 잘라져서 나오는 것 같다....
- {{임은상}}: LangGraph 그래프 실행을 “상태 변화/전체 상태/토큰(messages)/커스텀 이벤트(custom)/디버그”까지 다양한 모드로 스트리밍하면서, 병렬 노드·서브그래프·외부(OpenAI) 토큰 스트림도 체험해볼 수 있었다.
- {{홍자영}}: langgraph는 어렵다 - invoke는 다 된 걸 패키지로 한방에 보내주는 거지만 stream은 동영상 서비스 마냥 좀좀따리 보내줄 수가 있다
- {{곽수영}}: 랭그래프 updates / values / custom / messages / debug
- {{윤지연}}: 스트리밍은 출력이 아니라, 그래프 실행을 어떤 관점에서 관찰하느냐의 문제다.

## 자주 나온 질문
- Q1.
  - A.
  - 오늘 알고 넘어갈 것
  - invoke vs stream
  - stream() vs astream()
  - stream 5개의 모드가 있다 내맘대로 골라쓸 수 있다.
  - messages스트림일때 metadata로 필터링해서 보고 싶은것만 볼 수 있다.
  - LangChain을 안 쓰는 LLM이라도, 토큰 스트리밍을 custom 스트림으로 LangGraph에 끼워 넣을 수 있다.
  - subgraphs=True 로 서브그래프도 스트리밍 할 수 있다.
  
- Q2. 잘라져서 나오는 것을 보고 무엇에 활용하나요?
  - A. 
    우선 stream을 쓰는 이유는 invoke일때는 모든게 다 끝나야만 결과를 받아볼 수 있는데, stream모드로 하면 지금 어떤 식으로 진행되는지 중간 중간 확인할 수 있다는 장점이 있기 때문입니다. 그럼 이게 왜 필요하냐면요. 결과가 나오기 전에도, 지금 상황에 맞는 행동을 하기 위해서입니다.
  - 실행 중 / 툴 호출 중 / 승인 대기 구분 가능 
  - interrupt 발생 즉시 승인·거부·수정·중단 
  - 멈춘 위치·깨진 단계 즉시 확인 
  - appendix 
  - stream의 messages를 쓰면서 print(chunk["content"], end="|", flush=True) 이 명령어를 쓰면 한 토큰씩 나오는 걸 볼 수 있죠. 저기서 end="|" 이건 한 토큰 씩 나온다는걸 제가 강조하기 위해서 넣은거고, 실제 서비스에서 제공할때는 비워두는게 훨씬 자연스럽겠죠.

- Q3. LangChain 없이 OpenAI streaming 결과를 LangGraph custom으로 끼워 넣는 핵심 아이디어는 뭔가요?
  - A.
  - 예시를 그거로 보여드린거고, 중요한 건 랭체인 프레임워크를 안 써도 커스텀하면 스트림 할 수 있다. 예를 들면 로컬 llm이 될 수 있을것 같아요. 그런것도 custom만 하면 볼 쑤 있다

- Q4. messages가 의미 있으려면?
  - A.
  - 그 모드의 의미를 말씀하시는거죠. 오늘 예제 기준으론 2가지가 될 것 같아요. 
  - 1. Metadata기준으로 보고싶은것만 스트리밍하고 싶을때
  - 2. 유저가 멍하니 기다리는게 아니라 한 토큰씩 뽑아 나오는걸 보면서 안정감을 느끼는 ui발전

