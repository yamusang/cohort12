"""
.stream() : ë™ê¸°
.astream() : ë¹„ë™ê¸°

| ëª¨ë“œ        | ì˜ë¯¸                                         | ì¶œë ¥ ì˜ˆì‹œ
| ---------- | ------------------------------------------- | ------------------------------- |
| `updates`  | ê° ìŠ¤í…(step)ì—ì„œì˜ ìƒíƒœ ë³€ê²½ë§Œ ìŠ¤íŠ¸ë¦¬ë°í•¨      | {'nodeA': {'field': 'value'}}
| `values`   | ì „ì²´ ìƒíƒœë¥¼ ê³„ì† ì¶”ì í•´ì•¼ í•  ë•Œ                 | ì „ì²´ ìƒíƒœ ê°ì²´ {...}
| `messages` | ì‹¤ì‹œê°„ ì±„íŒ…ì²˜ëŸ¼ í† í° ë‹¨ìœ„ ì¶œë ¥ì´ í•„ìš”í•  ë•Œ         | (í† í°ì¡°ê°, metadata)
| `custom`   | ë…¸ë“œ ë‚´ë¶€ì—ì„œ writer()ë¡œ ì§ì ‘ ë§Œë“  ì„ì˜ ìŠ¤íŠ¸ë¦¼ ì¶œë ¥   | {"progress":"50%"}
| `debug`    | ë””ë²„ê¹…ìš© ìƒì„¸ ì •ë³´ (ë…¸ë“œ id, ì‹œê°„, ì…ë ¥/ì¶œë ¥ ë“±)     | ìƒì„¸ ì‹¤í–‰ ë¡œê·¸

"""
from dotenv import load_dotenv
load_dotenv()

# =========================================================
# ğŸ­ ì‹¤ì‹œê°„ AI ìŠ¤í† ë¦¬í…”ëŸ¬ (ë©”ì¸ ë°ëª¨)
# - 3ëª…ì˜ AI ì‘ê°€ê°€ ë³‘ë ¬ë¡œ ìŠ¤í† ë¦¬, êµí›ˆ, ì œëª© ìƒì„±
# - metadata í•„í„°ë§ìœ¼ë¡œ ê° ì‘ê°€ êµ¬ë¶„
# - ì¥ë¥´ ì„ íƒ + ìµœì¢… ê²°ê³¼ ì •ë¦¬ + íŒŒì¼ ì €ì¥ ê¸°ëŠ¥
# - gemini-2.5-flash ëª¨ë¸ ì‚¬ìš©
# =========================================================

import asyncio
from typing import TypedDict
from langgraph.graph import StateGraph, START, END
from langchain_google_genai import ChatGoogleGenerativeAI

# ì¥ë¥´ë³„ í”„ë¡¬í”„íŠ¸ ìŠ¤íƒ€ì¼
GENRES = {
    "1": ("íŒíƒ€ì§€", "ë§ˆë²•ê³¼ ì‹ ë¹„ë¡œìš´ ì„¸ê³„ê´€ì„ ë‹´ì•„"),
    "2": ("ë¡œë§¨ìŠ¤", "ì‚¬ë‘ê³¼ ê°ë™ì„ ë‹´ì•„"),
    "3": ("ê³µí¬", "ì„¬ëœ©í•˜ê³  ê¸´ì¥ê° ìˆê²Œ"),
    "4": ("ì½”ë¯¸ë””", "ìœ ë¨¸ëŸ¬ìŠ¤í•˜ê³  ì›ƒê¸°ê²Œ"),
    "5": ("ê°ë™", "ëˆˆë¬¼ ë‚˜ê²Œ ê°ë™ì ìœ¼ë¡œ"),
}

# 3ëª…ì˜ AI ì‘ê°€ (tagsë¡œ êµ¬ë¶„) - ëª¨ë‘ gemini-2.5-flash ì‚¬ìš©
story_writer = ChatGoogleGenerativeAI(model="gemini-2.5-flash", tags=["story"])
moral_writer = ChatGoogleGenerativeAI(model="gemini-2.5-flash", tags=["moral"])
title_writer = ChatGoogleGenerativeAI(model="gemini-2.5-flash", tags=["title"])

class StoryState(TypedDict):
    topic: str
    genre: str
    genre_style: str
    story: str
    moral: str
    title: str

# ìŠ¤í† ë¦¬ ì‘ê°€
async def write_story(state: StoryState, config):
    res = await story_writer.ainvoke(
        [{"role": "user", "content": f"{state['topic']}ì— ëŒ€í•œ ì§§ì€ ì´ì•¼ê¸°ë¥¼ {state['genre_style']} 3ë¬¸ì¥ìœ¼ë¡œ ì¨ì¤˜. í•œêµ­ì–´ë¡œ ë‹µë³€í•´."}],
        config,
    )
    return {"story": res.content}

# êµí›ˆ ì‘ê°€
async def write_moral(state: StoryState, config):
    res = await moral_writer.ainvoke(
        [{"role": "user", "content": f"{state['topic']}ì— ëŒ€í•œ êµí›ˆì„ {state['genre_style']} 1ë¬¸ì¥ìœ¼ë¡œ ì¨ì¤˜. í•œêµ­ì–´ë¡œ ë‹µë³€í•´."}],
        config,
    )
    return {"moral": res.content}

# ì œëª© ì‘ê°€
async def write_title(state: StoryState, config):
    res = await title_writer.ainvoke(
        [{"role": "user", "content": f"{state['topic']}ì— ëŒ€í•œ {state['genre']} ì¥ë¥´ì˜ ë§¤ë ¥ì ì¸ ì œëª©ì„ 1ê°œë§Œ ì¨ì¤˜. í•œêµ­ì–´ë¡œ ë‹µë³€í•´."}],
        config,
    )
    return {"title": res.content}

# ê·¸ë˜í”„ (3ëª…ì´ ë³‘ë ¬ë¡œ ì‘ì—…)
storyteller_graph = (
    StateGraph(StoryState)
    .add_node("write_story", write_story)
    .add_node("write_moral", write_moral)
    .add_node("write_title", write_title)
    .add_edge(START, "write_story")   # ë™ì‹œ ì‹¤í–‰
    .add_edge(START, "write_moral")   # ë™ì‹œ ì‹¤í–‰
    .add_edge(START, "write_title")   # ë™ì‹œ ì‹¤í–‰
    .compile()
)

# ë©”ì¸ ì‹¤í–‰
async def run_storyteller():
    print("\n" + "="*60)
    print("ğŸ­ ì‹¤ì‹œê°„ AI ìŠ¤í† ë¦¬í…”ëŸ¬")
    print("="*60)

    # ì¥ë¥´ ì„ íƒ
    print("\nğŸ“š ì¥ë¥´ë¥¼ ì„ íƒí•˜ì„¸ìš”:")
    for key, (name, _) in GENRES.items():
        print(f"  {key}. {name}")

    while True:
        genre_choice = input("\nì„ íƒ (1-5): ").strip()
        if genre_choice in GENRES:
            break
        print("âŒ 1~5 ì¤‘ì—ì„œ ì„ íƒí•´ì£¼ì„¸ìš”.")

    genre_name, genre_style = GENRES[genre_choice]
    print(f"âœ… '{genre_name}' ì¥ë¥´ê°€ ì„ íƒë˜ì—ˆìŠµë‹ˆë‹¤!")

    topic = input("\nì´ì•¼ê¸° ì£¼ì œë¥¼ ì…ë ¥í•˜ì„¸ìš”: ").strip()

    print(f"\nğŸ“– [{genre_name}] '{topic}'ì— ëŒ€í•œ ì´ì•¼ê¸°ë¥¼ 3ëª…ì˜ ì‘ê°€ê°€ ì‘ì„± ì¤‘...\n")
    print("-"*60)

    # ê²°ê³¼ ì €ì¥ìš© ë²„í¼
    story_buffer = ""
    moral_buffer = ""
    title_buffer = ""

    async for msg, metadata in storyteller_graph.astream(
        {
            "topic": topic,
            "genre": genre_name,
            "genre_style": genre_style,
        },
        stream_mode="messages",
    ):
        if not msg.content:
            continue

        # ì‘ê°€ë³„ë¡œ ë‹¤ë¥¸ ì´ëª¨ì§€ë¡œ ì¶œë ¥ + ë²„í¼ì— ì €ì¥
        tags = metadata.get("tags", [])

        if "story" in tags:
            print(f"ğŸ“ {msg.content}", end="", flush=True)
            story_buffer += msg.content
        elif "moral" in tags:
            print(f"\nğŸ’¡ {msg.content}", end="", flush=True)
            moral_buffer += msg.content
        elif "title" in tags:
            print(f"\nğŸ¬ {msg.content}", end="", flush=True)
            title_buffer += msg.content

    print("\n" + "-"*60)

    # ìµœì¢… ê²°ê³¼ ì •ë¦¬
    print("\n" + "="*60)
    print("ğŸ“œ ìµœì¢… ê²°ê³¼")
    print("="*60)
    print(f"\nğŸ·ï¸  ì¥ë¥´: {genre_name}")
    print(f"ğŸ¬ ì œëª©: {title_buffer.strip()}")
    print(f"\nğŸ“ ìŠ¤í† ë¦¬:\n{story_buffer.strip()}")
    print(f"\nğŸ’¡ êµí›ˆ: {moral_buffer.strip()}")
    print("="*60)

    # ì €ì¥ ì˜µì…˜
    save_choice = input("\nğŸ’¾ íŒŒì¼ë¡œ ì €ì¥í• ê¹Œìš”? (y/n): ").strip().lower()
    if save_choice == 'y':
        filename = f"story_{topic}_{genre_name}.txt"
        with open(filename, 'w', encoding='utf-8') as f:
            f.write(f"ğŸ­ AI ìŠ¤í† ë¦¬í…”ëŸ¬ - {genre_name}\n")
            f.write("="*40 + "\n\n")
            f.write(f"ì£¼ì œ: {topic}\n")
            f.write(f"ì œëª©: {title_buffer.strip()}\n\n")
            f.write(f"ìŠ¤í† ë¦¬:\n{story_buffer.strip()}\n\n")
            f.write(f"êµí›ˆ: {moral_buffer.strip()}\n")
        print(f"âœ… '{filename}' ì €ì¥ ì™„ë£Œ!")

    print("\nğŸ­ ìŠ¤í† ë¦¬í…”ë§ ì¢…ë£Œ!")
    print("="*60)

# ìŠ¤í† ë¦¬í…”ëŸ¬ ì‹¤í–‰
asyncio.run(run_storyteller())


# =========================================================
# ì•„ë˜ëŠ” ì›ë³¸ ì˜ˆì œ ì½”ë“œë“¤ (ì°¸ê³ ìš©)
# =========================================================

# =========================================================
# 1. ê¸°ë³¸ ìŠ¤íŠ¸ë¦¬ë° (updates / values / custom / messages / debug)
# =========================================================
print("\n\n" + "="*60)
print("ğŸ“š ì›ë³¸ ì˜ˆì œ ì½”ë“œ ì‹¤í–‰")
print("="*60)

from langgraph.config import get_stream_writer

llm = ChatGoogleGenerativeAI(model="gemini-2.5-flash") # streaming=Falseë„ ê°€ëŠ¥

class State(TypedDict):
    topic: str
    joke: str

def refine_topic(state: State):
    return {"topic": state["topic"] + " ê·¸ë¦¬ê³  ê³ ì–‘ì´"}

def generate_joke(state: State):
    msg = llm.invoke(f"{state['topic']}ì— ëŒ€í•œ ë†ë‹´ í•˜ë‚˜ ë§Œë“¤ì–´ì¤˜")
    # for custom stream
    writer = get_stream_writer()  
    writer(f"[1/2] {state['topic']} ì¡°íšŒ ì‹œì‘")
    writer(f"[2/2] {state['topic']} ì¡°íšŒ ì™„ë£Œ")
    return {"joke": f"ì´ê²ƒì€ {state['topic']}ì— ëŒ€í•œ ë†ë‹´ì…ë‹ˆë‹¤: \n{msg.content}"}

graph = (
    StateGraph(State)
    .add_node(refine_topic)
    .add_node(generate_joke)
    .add_edge(START, "refine_topic")
    .add_edge("refine_topic", "generate_joke")
    .add_edge("generate_joke", END)
    .compile()
)

# Basic usage (ê¸°ë³¸ì€ updates ëª¨ë“œ)
print("\n#######Basic usage#######")
for chunk in graph.stream(  
    {"topic": "ice cream"}
):
    print(chunk)

# Multiple modes
print("\n#######Multiple modes#######")
for mode, chunk in graph.stream({"topic": "ice cream"}, stream_mode=["updates", "custom", "values", "messages", "debug"]):
    print(f"\n{mode}: {chunk}")


# =========================================================
# ì•„ë˜ ì˜ˆì œë“¤ì€ GPT/OpenAI APIë¥¼ ì‚¬ìš©í•˜ë¯€ë¡œ ì£¼ì„ ì²˜ë¦¬ë¨
# í•„ìš”ì‹œ OpenAI API í‚¤ ì„¤ì • í›„ ì£¼ì„ í•´ì œí•˜ì—¬ ì‚¬ìš©
# =========================================================

"""
# =========================================================
# 2. messages ìŠ¤íŠ¸ë¦¼ + metadata í•„í„°ë§ (ëˆ„ê°€ / ì–´ë””ì„œ ë§í–ˆëŠ”ì§€)
# =========================================================
print(f"\\n\\n# -------------------------\\n# metadataë¥¼ ì´ìš©í•œ í•„í„°ë§\\n# -------------------------")

import asyncio
from typing import TypedDict
from langgraph.graph import START, StateGraph
from langchain.chat_models import init_chat_model

# 1. LLM í˜¸ì¶œë§ˆë‹¤ tagsë¥¼ ë‹¬ì•„ "ëˆ„ê°€ ë§í–ˆëŠ”ì§€" êµ¬ë¶„
joke_model = init_chat_model(model="gpt-4o-mini", tags=["joke"])
poem_model = init_chat_model(model="gpt-4o-mini", tags=["poem"])

class State(TypedDict):
    topic: str
    joke: str
    poem: str

# 2. ë…¸ë“œ 2ê°œ (node ê¸°ì¤€ í•„í„°ìš©)
async def write_joke(state: State, config):
    res = await joke_model.ainvoke(
        [{"role": "user", "content": f"{state['topic']}ì— ëŒ€í•œ ë†ë‹´ í•˜ë‚˜ ë§Œë“¤ì–´ì¤˜"}],
        config,
    )
    return {"joke": res.content}

async def write_poem(state: State, config):
    res = await poem_model.ainvoke(
        [{"role": "user", "content": f"{state['topic']}ì— ëŒ€í•œ ì§§ì€ ì‹œ í•˜ë‚˜ ë§Œë“¤ì–´ì¤˜"}],
        config,
    )
    return {"poem": res.content}

# 3. ê·¸ë˜í”„ (ë³‘ë ¬ ì‹¤í–‰)
graph = (
    StateGraph(State)
    .add_node("write_joke", write_joke)
    .add_node("write_poem", write_poem)
    .add_edge(START, "write_joke")
    .add_edge(START, "write_poem")
    .compile()
)


print("\\n######## messages ìŠ¤íŠ¸ë¦¼ + metadata í•„í„°ë§ ########")
# 4. ìŠ¤íŠ¸ë¦¬ë° ì¶œë ¥
async def main():
    async for msg, metadata in graph.astream(
        {"topic": "ê³ ì–‘ì´"},
        stream_mode="messages",
    ):
        if not msg.content:
            continue

        # Filter by LLM invocation (tags) : ì–´ë–¤ LLM í˜¸ì¶œì—ì„œ
        if metadata.get("tags") == ["joke"]:
            print(msg.content, end="", flush=True)

        # # Filter by node : ê·¸ë˜í”„ì˜ ì–´ëŠ ë…¸ë“œì—ì„œ
        # if metadata.get("langgraph_node") == "write_poem":
        #     print(msg.content, end="|", flush=True)

asyncio.run(main())


# =========================================================
# 3. LangChainì„ ì•ˆ ì“°ëŠ” LLMì´ë¼ë„, í† í° ìŠ¤íŠ¸ë¦¬ë°ì„ custom ìŠ¤íŠ¸ë¦¼ìœ¼ë¡œ LangGraphì— ë¼ì›Œ ë„£ì„ ìˆ˜ ìˆìŒ
# =========================================================
import operator
import json

from typing import TypedDict
from typing_extensions import Annotated
from langgraph.graph import StateGraph, START

from openai import AsyncOpenAI

openai_client = AsyncOpenAI()
model_name = "gpt-4o-mini"


async def stream_tokens(model_name: str, messages: list[dict]):
    response = await openai_client.chat.completions.create(
        messages=messages, model=model_name, stream=True
    )
    role = None
    async for chunk in response: # ë¹„ë™ê¸° ë°˜ë³µë¬¸ìœ¼ë¡œ í† í° ì¡°ê°ì„ í•˜ë‚˜ì”© ë°›ìŒ
        delta = chunk.choices[0].delta

        if delta.role is not None:
            role = delta.role

        if delta.content: # í† í° ë‚´ìš©ì´ ìˆë‹¤ë©´ yieldë¡œ ë°–ìœ¼ë¡œ ë³´ëƒ„
            yield {"role": role, "content": delta.content}


# this is our tool
async def get_items(place: str) -> str:
    writer = get_stream_writer()
    response = ""
    async for msg_chunk in stream_tokens(
        model_name,
        [{"role": "user", "content": f"{place}ì—ì„œ ë³¼ ìˆ˜ ìˆëŠ” ë¬¼ê±´ 3ê°€ì§€ë¥¼ ì„¤ëª…ê³¼ í•¨ê»˜ ì•Œë ¤ì¤˜"}],
    ):
        response += msg_chunk["content"]
        writer(msg_chunk)

    return response


class State(TypedDict):
    messages: Annotated[list[dict], operator.add]


# this is the tool-calling graph node
async def call_tool(state: State):
    ai_message = state["messages"][-1]
    tool_call = ai_message["tool_calls"][-1]

    function_name = tool_call["function"]["name"]
    if function_name != "get_items":
        raise ValueError(f"Tool {function_name} not supported")

    function_arguments = tool_call["function"]["arguments"]
    arguments = json.loads(function_arguments)

    function_response = await get_items(**arguments)
    tool_message = {
        "tool_call_id": tool_call["id"],
        "role": "tool",
        "name": function_name,
        "content": function_response,
    }
    return {"messages": [tool_message]}


graph = (
    StateGraph(State)
    .add_node(call_tool)
    .add_edge(START, "call_tool")
    .compile()
)

print("\\n\\n######## custom ìŠ¤íŠ¸ë¦¼ (LangChain ì—†ì´ í† í° ìŠ¤íŠ¸ë¦¬ë°) ########")
inputs = {
    "messages": [
        {
            "content": None,
            "role": "assistant",
            "tool_calls": [
                {
                    "id": "1",
                    "function": {
                        "arguments": '{"place":"bedroom"}',
                        "name": "get_items",
                    },
                    "type": "function",
                }
            ],
        }
    ]
}

import asyncio
async def main():
    async for chunk in graph.astream(
        inputs,
        stream_mode="custom",
    ):
        print(chunk["content"], end="|", flush=True)

asyncio.run(main())
"""

# =========================================================
# 4. Subgraph ìŠ¤íŠ¸ë¦¬ë° (Gemini ì‚¬ìš©í•˜ì§€ ì•ŠìŒ, ìˆœìˆ˜ ë¡œì§ë§Œ)
# =========================================================
print("\n\n######## Subgraph ìŠ¤íŠ¸ë¦¬ë° ########")
from langgraph.graph import START, StateGraph
from typing import TypedDict

# Define subgraph
class SubgraphState(TypedDict):
    foo: str  # note that this key is shared with the parent graph state
    bar: str

def subgraph_node_1(state: SubgraphState):
    return {"bar": "bar"}

def subgraph_node_2(state: SubgraphState):
    return {"foo": state["foo"] + state["bar"]}

subgraph_builder = StateGraph(SubgraphState)
subgraph_builder.add_node(subgraph_node_1)
subgraph_builder.add_node(subgraph_node_2)
subgraph_builder.add_edge(START, "subgraph_node_1")
subgraph_builder.add_edge("subgraph_node_1", "subgraph_node_2")
subgraph = subgraph_builder.compile()

# Define parent graph
class ParentState(TypedDict):
    foo: str

def node_1(state: ParentState):
    return {"foo": "hi! " + state["foo"]}

builder = StateGraph(ParentState)
builder.add_node("node_1", node_1)
builder.add_node("node_2", subgraph)
builder.add_edge(START, "node_1")
builder.add_edge("node_1", "node_2")
graph = builder.compile()

for chunk in graph.stream(
    {"foo": "foo"},
    stream_mode="updates",
    # Set subgraphs=True to stream outputs from subgraphs
    subgraphs=True,  
):
    print(chunk)