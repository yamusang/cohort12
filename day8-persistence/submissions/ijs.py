"""
ì²´í¬í¬ì¸í„°ë¥¼ ë¶™ì—¬ ì»´íŒŒì¼í•˜ë©´, ê·¸ë˜í”„ ìƒíƒœê°€ ë§¤ super-stepë§ˆë‹¤ ìë™ ì €ì¥(ì²´í¬í¬ì¸íŠ¸) ëœë‹¤.
ì €ì¥ëœ ì²´í¬í¬ì¸íŠ¸ëŠ” thread(ìŠ¤ë ˆë“œ) ì— ìŒ“ì´ë©°, ì‹¤í–‰ì´ ëë‚œ ë’¤ì—ë„ ìƒíƒœ ì¡°íšŒ/ì¬ê°œ/ë¶„ê¸°ê°€ ê°€ëŠ¥í•´ì§„ë‹¤.
ê·¸ ê²°ê³¼ë¡œ HITL(ì¤‘ê°„ì— ì‚¬ëŒ ê°œì…), ëŒ€í™”/ì„¸ì…˜ ë©”ëª¨ë¦¬, ê³¼ê±° ì‹œì  ì¬ìƒ(íƒ€ì„ íŠ¸ë˜ë¸”), ì‹¤íŒ¨ ì§€ì ë¶€í„° ë³µêµ¬(ì¥ì•  ë‚´ì„±) ê°™ì€ ê¸°ëŠ¥ì´ ì—´ë¦°ë‹¤.
Agent Serverë¥¼ ì“°ë©´ ì´ëŸ° ì €ì¥/ê´€ë¦¬(ì²´í¬í¬ì¸íŒ…)ë¥¼ ì„œë²„ê°€ ìë™ìœ¼ë¡œ í•´ì¤˜ì„œ ì§ì ‘ ì„¤ì •í•  í•„ìš”ê°€ ì—†ë‹¤


thread
threadëŠ” ê·¸ë˜í”„ ì‹¤í–‰ ìƒíƒœë¥¼ ë¬¶ëŠ” ê³ ìœ  ì‹ë³„ì(ID)
thread_idëŠ” ì´ ê·¸ë˜í”„ ì‹¤í–‰ì˜ ê¸°ì–µì´ ì €ì¥ë˜ëŠ” ëŒ€í™”ë°©/ì„¸ì…˜ í‚¤
"""

#----------------------------------------
# ğŸ‹ï¸ ê¸°ê°€ì°¨ë“œ AI ìƒë‹´ì‚¬ (GigaChad Counselor)
# - ëƒ‰ì² í•˜ê³  ì§ì„¤ì ì¸ ì¡°ì–¸
# - ê³¼ê±° ê³ ë¯¼ ì´ë ¥ ì¶”ì 
# - ë°˜ë³µ ê³ ë¯¼ ì‹œ ë” ê°•í•œ í”¼ë“œë°±
# - ëª¨ë“  ëª¨ë¸ì„ gemini-2.5-flashë¡œ í†µì¼
#----------------------------------------

#----------------------------------------
#Checkpoints (ìŠ¤ë ˆë“œ(thread)ì˜ íŠ¹ì • ì‹œì  ìƒíƒœë¥¼ ì²´í¬í¬ì¸íŠ¸(checkpoint))
"""
config: í•´ë‹¹ ì²´í¬í¬ì¸íŠ¸ì™€ ì—°ê´€ëœ ì„¤ì •(config)
metadata: ì²´í¬í¬ì¸íŠ¸ì— ëŒ€í•œ ë©”íƒ€ë°ì´í„°
values: í•´ë‹¹ ì‹œì ì˜ state ê°’
next: ë‹¤ìŒì— ì‹¤í–‰ë  ë…¸ë“œ
tasks: ë‹¤ìŒì— ì‹¤í–‰ë  ì‘ì—… ì •ë³´
"""
#----------------------------------------

from typing import Annotated
from typing_extensions import TypedDict
from operator import add
from langgraph.graph import StateGraph, START, END
from langgraph.checkpoint.memory import InMemorySaver
from langchain_core.runnables import RunnableConfig

class State(TypedDict):
    foo: str
    bar: Annotated[list[str], add]

def node_a(state: State):
    return {"foo": "a", "bar": ["a"]}

def node_b(state: State):
    return {"foo": "b", "bar": ["b"]}

workflow = StateGraph(State)
workflow.add_node(node_a)
workflow.add_node(node_b)
workflow.add_edge(START, "node_a")
workflow.add_edge("node_a", "node_b")
workflow.add_edge("node_b", END)

# Checkpointer ì„¤ì •
checkpointer = InMemorySaver()
graph = workflow.compile(checkpointer=checkpointer)

# ì´ˆê¸° ì‹¤í–‰
config: RunnableConfig = {"configurable": {"thread_id": "1"}}
print("--- [ì´ˆê¸° ì‹¤í–‰ ì‹œì‘] ---")
result=graph.invoke({"foo": "", "bar": []}, config)
print("ì´ˆê¸° ì‹¤í–‰ ì™„ë£Œ\n")
print(result) # {'foo': 'b', 'bar': ['a', 'b']}


##1. Get state (ìƒíƒœ ì¡°íšŒ)##
config = {"configurable": {"thread_id": "1"}} #{"thread_id": "1", "checkpoint_id": } íŠ¹ì • ì²´í¬í¬ì¸íŠ¸ ì¡°íšŒ ê°€ëŠ¥
latest_state = graph.get_state(config)
print("\n## ìµœì‹  ìƒíƒœ ì¡°íšŒ ##")
print(latest_state)

##2. Get state history (ìƒíƒœ íˆìŠ¤í† ë¦¬ ì¡°íšŒ)##
print("\n## ìƒíƒœ íˆìŠ¤í† ë¦¬ ì¡°íšŒ ##")
config = {"configurable": {"thread_id": "1"}}
history = list(graph.get_state_history(config))
print(history) # ë‚´ë¦¼ì°¨ìˆœ, checkpoint_idëŠ” ë§¤ë²ˆ ë³€ê²½ ë¨

##3. Replay (ë¦¬í”Œë ˆì´)##
config = {"configurable": {"thread_id": "1", "checkpoint_id": history[1].config['configurable']['checkpoint_id']}}
graph.invoke(None, config=config)
print("\n## ë¦¬í”Œë ˆì´ ê²°ê³¼ ##")
print(graph.get_state(config))


##4. Update state (ìƒíƒœ ìˆ˜ì •)##
"""
config
thread_id: ì—…ë°ì´íŠ¸í•  ìŠ¤ë ˆë“œ
checkpoint_id(ì„ íƒ): í•´ë‹¹ ì²´í¬í¬ì¸íŠ¸ì—ì„œ ìƒíƒœë¥¼ í¬í¬(fork)
"""

"""
values
ì—…ë°ì´íŠ¸í•  state ê°’
ë…¸ë“œê°€ stateë¥¼ ì—…ë°ì´íŠ¸í•˜ëŠ” ê²ƒê³¼ ë™ì¼í•˜ê²Œ ì²˜ë¦¬
reducer ê·œì¹™ì— ë”°ë¼ ì²˜ë¦¬ë¨
"""

config = {"configurable": {"thread_id": "1"}}
graph.update_state(config, {"foo": "2", "bar": ["b"]})
print("\n## Update State ê²°ê³¼ ##")
print(graph.get_state(config))

"""
as_node
update_stateë¥¼ í˜¸ì¶œí•  ë•Œ ì„ íƒì ìœ¼ë¡œ as_nodeë¥¼ ì§€ì •, ì„ íƒ ì—†ìœ¼ë©´ ìµœì‹ ìœ¼ë¡œ
"""
# íŠ¹ì • ë…¸ë“œê°€ ì—…ë°ì´íŠ¸í•œ ê²ƒì²˜ëŸ¼ ì†ì—¬ì„œ ë‹¤ìŒ ì‹¤í–‰ íë¦„ì„ ì œì–´í•  ìˆ˜ ìˆìŒ
print("\n## 4-1. Update State with as_node ##")
graph.update_state(config, {"foo": "forked_foo", "bar": ["hello world"]}, as_node="node_a")
forked_state = graph.get_state(config)
print(f"as_node='node_a' ì—…ë°ì´íŠ¸ í›„ ë‹¤ìŒì— ì‹¤í–‰ë  ë…¸ë“œ(next): {forked_state.next}")

print("\n## ì¬ê°œ ì‹¤í–‰ í›„ ìƒíƒœ ##")
graph.invoke(None, config={"configurable": {"thread_id": "1"}})
print(graph.get_state({"configurable": {"thread_id": "1"}}))

#----------------------------------------
#Memory store
"""
StoreëŠ” â€œìŠ¤ë ˆë“œë¥¼ ë„˜ì–´ ê³µìœ ë˜ëŠ” ì‚¬ìš©ì/ì „ì—­ ê¸°ì–µâ€ì„ ë‹´ë‹¹
"""
#----------------------------------------

##1. Basic Usage##
import uuid
from langgraph.store.memory import InMemoryStore
in_memory_store = InMemoryStore()

user_id = "1"
namespace_for_memory = (user_id, "memories") #1ë²ˆ ì‚¬ìš©ì memories í´ë”
print("\n## 1. Basic Usage ##")
print(f"ë„¤ì„ìŠ¤í˜ì´ìŠ¤: {namespace_for_memory}")

memory_id = str(uuid.uuid4())
memory = {"ìŒì‹ì„ í˜¸ë„" : "í”¼ì ì¢‹ì•„"}
in_memory_store.put(namespace_for_memory, memory_id, memory) #ì €ì¥ put

memories = in_memory_store.search(namespace_for_memory) #ì¡°íšŒ search
print(f"ë©”ëª¨ë¦¬ì— ë‹´ê¸´ ë°ì´í„°: {memories[-1].dict()}")


##2. Semantic Search (ì„ë² ë”© ì—†ì´ ì‚¬ìš©)##
from dotenv import load_dotenv
import os

# .env íŒŒì¼ ë¡œë“œ (ìƒìœ„ ë””ë ‰í† ë¦¬)
load_dotenv(os.path.join(os.path.dirname(__file__), '..', '..', '.env'))

# ì„ë² ë”© ì—†ì´ InMemoryStore ì‚¬ìš© (API í• ë‹¹ëŸ‰ ì´ìŠˆ íšŒí”¼)
store = InMemoryStore()

print("\n## 2. Semantic Search ##")

#(1) ì•„ì§ ë©”ëª¨ë¦¬ ì—†ì„ ë•Œ ê²€ìƒ‰
memories = store.search(
    namespace_for_memory,
    limit=3  # Return top 3 matches
)
print("ì´ˆê¸° ê²€ìƒ‰ ê²°ê³¼:", [m.value for m in memories])

# íŠ¹ì • í•„ë“œë§Œ ì„ë² ë”©
store.put(
    namespace_for_memory,
    str(uuid.uuid4()),
    {
        "ìŒì‹ì„ í˜¸ë„": "ë‚˜ëŠ” ì´íƒˆë¦¬ì•„ ìŒì‹ì„ ì¢‹ì•„í•´",
        "context": "ì €ë…ì— ê´€í•œ ê³„íš ë…¼ì˜í•˜ê¸°"
    },
    index=["ìŒì‹ì„ í˜¸ë„"] # Only embed "ìŒì‹ì„ í˜¸ë„" field
)

# ì„ë² ë”© ì—†ì´ ì €ì¥ (ì¡°íšŒëŠ” ê°€ëŠ¥, ì‹œë§¨í‹± ê²€ìƒ‰ì€ ë¶ˆê°€)
store.put(
    namespace_for_memory,
    str(uuid.uuid4()),
    {
        "ì·¨ë¯¸": "ìŠ¤í‚¨ìŠ¤ì¿ ë²„ ë‹¤ì´ë¹™ì´ ì·¨ë¯¸ì•¼",
        "context": "ì €ë…ì— ê´€í•œ ê³„íš ë…¼ì˜í•˜ê¸°"
    },
	index=False
)

store.put(
    namespace_for_memory,
    str(uuid.uuid4()),
    {
        "ìŒì‹ì„ í˜¸ë„": "ë‚˜ëŠ” ê³ ìˆ˜ê°€ ì‹«ì–´",
        "context": "ì ì‹¬ì— ê´€í•œ ê³„íš ë…¼ì˜í•˜ê¸°"
    }
)

#(2) ë©”ëª¨ë¦¬ ì¶”ê°€ í›„ ê²€ìƒ‰
memories = store.search(
    namespace_for_memory,
    limit=3  # Return top 3 matches
)
print("ìµœì¢… ê²€ìƒ‰ ê²°ê³¼:", [(m.value, m.score) for m in memories])


##3. Using in LangGraph##
print("\n## 3. LangGraphì—ì„œ Memory Store ì‚¬ìš©í•˜ê¸° ##")

import uuid
import operator
from typing_extensions import TypedDict, Annotated

from langchain_core.runnables import RunnableConfig
from langgraph.graph import StateGraph, START, END
from langgraph.checkpoint.memory import InMemorySaver
from langgraph.store.memory import InMemoryStore
from langgraph.store.base import BaseStore
from langchain.messages import SystemMessage, HumanMessage
from langchain_google_genai import ChatGoogleGenerativeAI

llm = ChatGoogleGenerativeAI(
    model="gemini-2.5-flash",
    temperature=0.9,  # ê¸°ê°€ì°¨ë“œ íŠ¹ìœ ì˜ ê°•ë ¬í•œ ë‹µë³€ì„ ìœ„í•´ ë†’ê²Œ ì„¤ì •
)

# thread_id ê¸°ì¤€ ëŒ€í™” ìƒíƒœ ì €ì¥
checkpointer = InMemorySaver()

# user_id ê¸°ì¤€ ì¥ê¸° ê¸°ì–µ ì €ì¥
store = InMemoryStore()

class State(TypedDict):
    # messagesëŠ” ë…¸ë“œ ì‹¤í–‰ ê²°ê³¼ê°€ ëˆ„ì ë¨
    messages: Annotated[list[dict], operator.add]

# ê³ ë¯¼ ì €ì¥ ë…¸ë“œ
def update_memory(state: State, config: RunnableConfig, *, store: BaseStore):
    user_id = config["configurable"]["user_id"]
    namespace = (user_id, "concerns")  # ê³ ë¯¼ ì €ì¥ì†Œ

    # ê°€ì¥ ìµœê·¼ ì‚¬ìš©ì ê³ ë¯¼
    last_user_message = state["messages"][-1]["content"]

    memory_id = str(uuid.uuid4())
    store.put(
        namespace,
        memory_id,
        {"concern": last_user_message},
    )

    return {}

# ğŸ‹ï¸ ê¸°ê°€ì°¨ë“œ ìƒë‹´ ë…¸ë“œ
def call_model(state: State, config: RunnableConfig, *, store: BaseStore):
    user_id = config["configurable"]["user_id"]
    namespace = (user_id, "concerns")

    user_text = state["messages"][-1]["content"]

    # ê³¼ê±° ë¹„ìŠ·í•œ ê³ ë¯¼ ê²€ìƒ‰ (ì„ë² ë”© ì—†ì´)
    past_concerns = store.search(
        namespace,
        limit=5,
    )

    # ë°˜ë³µ íŒ¨í„´ ë¶„ì„
    repeat_count = len(past_concerns)
    past_text = "\n".join([f"- {c.value['concern']}" for c in past_concerns]) if past_concerns else "(ì²« ìƒë‹´)"

    # ğŸ‹ï¸ ê¸°ê°€ì°¨ë“œ í˜ë¥´ì†Œë‚˜ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸
    system = SystemMessage(
        content=(
            "ë„ˆëŠ” 'ê¸°ê°€ì°¨ë“œ' ìŠ¤íƒ€ì¼ì˜ AI ìƒë‹´ì‚¬ì•¼.\n\n"
            "## ìºë¦­í„° íŠ¹ì§•:\n"
            "- ëƒ‰ì² í•˜ê³  ì§ì„¤ì ì´ë©°, ê°ì •ì— íœ˜ë‘˜ë¦¬ì§€ ì•ŠìŒ\n"
            "- í˜„ì‹¤ì ì´ê³  ì‹¤ìš©ì ì¸ ì¡°ì–¸\n"
            "- ë³€ëª…ì´ë‚˜ ìê¸° í•©ë¦¬í™”ë¥¼ ìš©ë‚©í•˜ì§€ ì•ŠìŒ\n"
            "- 'í•´ì•¼ í•œë‹¤'ê°€ ì•„ë‹ˆë¼ 'í•  ìˆ˜ ìˆë‹¤'ëŠ” ìì‹ ê° ì£¼ì…\n"
            "- ê°„ê²°í•˜ê³  ì„íŒ©íŠ¸ ìˆëŠ” ë¬¸ì¥\n"
            "- ê°€ë” ğŸ’ªğŸ”¥ğŸ˜ ì´ëª¨ì§€ ì‚¬ìš©\n"
            "- ì‚¬ìš©ìë¥¼ ì²˜ìŒ ë¶€ë¥¼ ë•Œ Oh, My sweet (ì‚¬ìš©ì ì´ë¦„) ì‚£ì‚! ì´ë¼ ìƒëƒ¥í•˜ê²Œ ë¶€ë¥´ê¸°\n"
            "- ì‚¬ìš©ìê°€ ì˜ëª»ë˜ê±°ë‚˜ ë¶€ì •ì ì¸ ìƒê°ì„ ê°€ì§ˆ ë•Œ You Puking Stop, (ì‚¬ìš©ì ì´ë¦„) ì‚£ì‚! ë¼ê³  ë‹¤ê·¸ì¹˜ê¸°\n\n"
            "## ìƒë‹´ ì›ì¹™:\n"
            "1. ë¬¸ì œì˜ ë³¸ì§ˆì„ ê¿°ëš«ì–´ ë´„\n"
            "2. ì‹¤í–‰ ê°€ëŠ¥í•œ êµ¬ì²´ì  í–‰ë™ ì œì‹œ\n"
            "3. ìì±…ë³´ë‹¤ëŠ” ì•ìœ¼ë¡œ ë‚˜ì•„ê°ˆ ë°©í–¥ ì œì‹œ\n"
            "4. í•„ìš”ì‹œ ë”°ë”í•œ ì¼ì¹¨\n\n"
            f"## ì‚¬ìš©ì ì •ë³´:\n"
            f"- ë°˜ë³µ ìƒë‹´ íšŸìˆ˜: {repeat_count}íšŒ\n"
            f"- ê³¼ê±° ê³ ë¯¼ë“¤:\n{past_text}\n\n"
            "## ì‘ë‹µ ì „ëµ:\n"
            "- 1íšŒ: ê³µê° + ëª…í™•í•œ í•´ê²°ì±…\n"
            "- 2-3íšŒ: íŒ¨í„´ ì§€ì  + ê°•í•œ ë™ê¸°ë¶€ì—¬\n"
            "- 4íšŒ ì´ìƒ: ì§ì„¤ì  í”¼ë“œë°± + ìµœí›„í†µì²©ì‹ ì¡°ì–¸\n\n"
            "ë°˜ë“œì‹œ í•œêµ­ì–´ë¡œ ë‹µë³€í•˜ê³ , 300ì ì´ë‚´ë¡œ ê°„ê²°í•˜ê²Œ."
        )
    )
    human = HumanMessage(content=user_text)

    ai_msg = llm.invoke([system, human])

    return {
        "messages": [
            {
                "role": "assistant",
                "content": ai_msg.content,
            }
        ]
    }

# ê·¸ë˜í”„ êµ¬ì„±
graph = StateGraph(State)

graph.add_node("update_memory", update_memory)
graph.add_node("call_model", call_model)

graph.add_edge(START, "update_memory")
graph.add_edge("update_memory", "call_model")
graph.add_edge("call_model", END)

graph = graph.compile(
    checkpointer=checkpointer,
    store=store,
)


# ğŸ‹ï¸ ê¸°ê°€ì°¨ë“œ ìƒë‹´ì‚¬ ì‹¤í–‰ ì˜ˆì‹œ
config = {
    "configurable": {
        "thread_id": "1",
        "user_id": "user_1",
    }
}

print("\n" + "="*60)
print("ğŸ‹ï¸ ê¸°ê°€ì°¨ë“œ AI ìƒë‹´ì‚¬ ì‹œì‘")
print("="*60)

# ê³ ë¯¼ ì‹œë‚˜ë¦¬ì˜¤
concerns = [
    "ìê¾¸ ë¯¸ë£¨ê²Œ ë¼ìš”. ì–´ë–»ê²Œ í•˜ì£ ?",
    "ìš´ë™ì„ ì‹œì‘í•˜ê³  ì‹¶ì€ë° ì˜ì§€ê°€ ì•ˆ ìƒê²¨ìš”",
    "ë˜ ë¯¸ë£¨ê²Œ ëì–´ìš”... ì € ì•ˆ ë³€í•˜ë‚˜ë´ìš”",
]

for i, concern in enumerate(concerns, 1):
    print(f"\n--- [{i}ë²ˆì§¸ ìƒë‹´] ---")
    print(f"ğŸ˜¢ ì‚¬ìš©ì: {concern}")
    print("-" * 40)

    for update in graph.stream(
        {"messages": [{"role": "user", "content": concern}]},
        config,
        stream_mode="updates",
    ):
        if "call_model" in update:
            response = update["call_model"]["messages"][-1]["content"]
            print(f"ğŸ‹ï¸ ê¸°ê°€ì°¨ë“œ: {response}")

print("\n" + "="*60)
print("ìƒë‹´ ì¢…ë£Œ")
print("="*60)


"""Apendix

- langgraph.json ì„¤ì •ì€ â€œë°°í¬/ì„œë²„ í™˜ê²½ì—ì„œ store ì¸ë±ìŠ¤ ì¼œê¸°â€
"store": {
  "index": {
    "embed": "openai:text-embeddings-3-small",
    "dims": 1536,
    "fields": ["$"]
  }

# ì‹œìŠ¤í…œì ìœ¼ë¡œ ì–´ë–»ê²Œ ë³´ê´€Â·ë³´í˜¸Â·ìš´ì˜í• ê¹Œ
 - langgraph-checkpoint vs langgraph-checkpoint-sqlite vs langgraph-checkpoint-postgres: 'ì–´ë””ì— ì €ì¥í•˜ëƒ' ì°¨ì´
 - Serializer: 'stateë¥¼ ì €ì¥ ê°€ëŠ¥í•˜ê²Œ ë³€í™˜'í•˜ëŠ” ê·œì¹™
 - pickle_fallback: ë³€í™˜ ì‹¤íŒ¨ ì‹œ pickleë¡œ êµ¬ì œ
 - EncryptedSerializer: ì €ì¥ë˜ëŠ” ë‚´ìš© ìì²´ë¥¼ ì•”í˜¸í™”
 """