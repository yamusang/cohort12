"""
ì²´í¬í¬ì¸í„°ë¥¼ ë¶™ì—¬ ì»´íŒŒì¼í•˜ë©´, ê·¸ë˜í”„ ìƒíƒœê°€ ë§¤ super-stepë§ˆë‹¤ ìë™ ì €ì¥(ì²´í¬í¬ì¸íŠ¸) ëœë‹¤.
ì €ì¥ëœ ì²´í¬í¬ì¸íŠ¸ëŠ” thread(ìŠ¤ë ˆë“œ) ì— ìŒ“ì´ë©°, ì‹¤í–‰ì´ ëë‚œ ë’¤ì—ë„ ìƒíƒœ ì¡°íšŒ/ì¬ê°œ/ë¶„ê¸°ê°€ ê°€ëŠ¥í•´ì§„ë‹¤.
ê·¸ ê²°ê³¼ë¡œ HITL(ì¤‘ê°„ì— ì‚¬ëŒ ê°œì…), ëŒ€í™”/ì„¸ì…˜ ë©”ëª¨ë¦¬, ê³¼ê±° ì‹œì  ì¬ìƒ(íƒ€ì„ íŠ¸ë˜ë¸”), ì‹¤íŒ¨ ì§€ì ë¶€í„° ë³µêµ¬(ì¥ì•  ë‚´ì„±) ê°™ì€ ê¸°ëŠ¥ì´ ì—´ë¦°ë‹¤.
Agent Serverë¥¼ ì“°ë©´ ì´ëŸ° ì €ì¥/ê´€ë¦¬(ì²´í¬í¬ì¸íŒ…)ë¥¼ ì„œë²„ê°€ ìë™ìœ¼ë¡œ í•´ì¤˜ì„œ ì§ì ‘ ì„¤ì •í•  í•„ìš”ê°€ ì—†ë‹¤


thread
threadëŠ” ê·¸ë˜í”„ ì‹¤í–‰ ìƒíƒœë¥¼ ë¬¶ëŠ” ê³ ìœ  ì‹ë³„ì(ID)
thread_idëŠ” ì´ ê·¸ë˜í”„ ì‹¤í–‰ì˜ ê¸°ì–µì´ ì €ì¥ë˜ëŠ” ëŒ€í™”ë°©/ì„¸ì…˜ í‚¤
"""

#----------------------------------------
#Checkpoints (ìŠ¤ë ˆë“œ(thread)ì˜ íŠ¹ì • ì‹œì  ìƒíƒœë¥¼ ì²´í¬í¬ì¸íŠ¸(checkpoint))
"""
config: í•´ë‹¹ ì²´í¬í¬ì¸íŠ¸ì™€ ì—°ê´€ëœ ì„¤ì •(config)
metadata: ì²´í¬í¬ì¸íŠ¸ì— ëŒ€í•œ ë©”íƒ€ë°ì´í„°
values: í•´ë‹¹ ì‹œì ì˜ state ê°’
next: ë‹¤ìŒì— ì‹¤í–‰ë  ë…¸ë“œ
tasks: ë‹¤ìŒì— ì‹¤í–‰ë  ì‘ì—… ì •ë³´
"""
#----------------------------------------

from typing import Annotated
from typing_extensions import TypedDict
from operator import add
from langgraph.graph import StateGraph, START, END
from langgraph.checkpoint.memory import InMemorySaver
from langchain_core.runnables import RunnableConfig

class State(TypedDict):
    foo: str
    bar: Annotated[list[str], add]

def node_a(state: State):
    return {"foo": "a", "bar": ["a"]}

def node_b(state: State):
    return {"foo": "b", "bar": ["b"]}

workflow = StateGraph(State)
workflow.add_node(node_a)
workflow.add_node(node_b)
workflow.add_edge(START, "node_a")
workflow.add_edge("node_a", "node_b")
workflow.add_edge("node_b", END)

# Checkpointer ì„¤ì •
checkpointer = InMemorySaver()
graph = workflow.compile(checkpointer=checkpointer)

# ì´ˆê¸° ì‹¤í–‰
config: RunnableConfig = {"configurable": {"thread_id": "1"}}
print("--- [ì´ˆê¸° ì‹¤í–‰ ì‹œì‘] ---")
result=graph.invoke({"foo": "", "bar": []}, config)
print("ì´ˆê¸° ì‹¤í–‰ ì™„ë£Œ\n")
print(result) # {'foo': 'b', 'bar': ['a', 'b']}


##1. Get state (ìƒíƒœ ì¡°íšŒ)##
config = {"configurable": {"thread_id": "1"}} #{"thread_id": "1", "checkpoint_id": } íŠ¹ì • ì²´í¬í¬ì¸íŠ¸ ì¡°íšŒ ê°€ëŠ¥
latest_state = graph.get_state(config)
print("\n## ìµœì‹  ìƒíƒœ ì¡°íšŒ ##")
print(latest_state)

##2. Get state history (ìƒíƒœ íˆìŠ¤í† ë¦¬ ì¡°íšŒ)##
print("\n## ìƒíƒœ íˆìŠ¤í† ë¦¬ ì¡°íšŒ ##")
config = {"configurable": {"thread_id": "1"}}
history = list(graph.get_state_history(config))
print(history) # ë‚´ë¦¼ì°¨ìˆœ, checkpoint_idëŠ” ë§¤ë²ˆ ë³€ê²½ ë¨

##3. Replay (ë¦¬í”Œë ˆì´)##
config = {"configurable": {"thread_id": "1", "checkpoint_id": history[1].config['configurable']['checkpoint_id']}}
graph.invoke(None, config=config)
print("\n## ë¦¬í”Œë ˆì´ ê²°ê³¼ ##")
print(graph.get_state(config))


##4. Update state (ìƒíƒœ ìˆ˜ì •)##
"""
config
thread_id: ì—…ë°ì´íŠ¸í•  ìŠ¤ë ˆë“œ
checkpoint_id(ì„ íƒ): í•´ë‹¹ ì²´í¬í¬ì¸íŠ¸ì—ì„œ ìƒíƒœë¥¼ í¬í¬(fork)
"""

"""
values
ì—…ë°ì´íŠ¸í•  state ê°’
ë…¸ë“œê°€ stateë¥¼ ì—…ë°ì´íŠ¸í•˜ëŠ” ê²ƒê³¼ ë™ì¼í•˜ê²Œ ì²˜ë¦¬
reducer ê·œì¹™ì— ë”°ë¼ ì²˜ë¦¬ë¨
"""

config = {"configurable": {"thread_id": "1"}}
graph.update_state(config, {"foo": "2", "bar": ["b"]})
print("\n## Update State ê²°ê³¼ ##")
print(graph.get_state(config))

"""
as_node
update_stateë¥¼ í˜¸ì¶œí•  ë•Œ ì„ íƒì ìœ¼ë¡œ as_nodeë¥¼ ì§€ì •, ì„ íƒ ì—†ìœ¼ë©´ ìµœì‹ ìœ¼ë¡œ
"""
# íŠ¹ì • ë…¸ë“œê°€ ì—…ë°ì´íŠ¸í•œ ê²ƒì²˜ëŸ¼ ì†ì—¬ì„œ ë‹¤ìŒ ì‹¤í–‰ íë¦„ì„ ì œì–´í•  ìˆ˜ ìˆìŒ
print("\n## 4-1. Update State with as_node ##")
graph.update_state(config, {"foo": "forked_foo", "bar": ["hello world"]}, as_node="node_a")
forked_state = graph.get_state(config)
print(f"as_node='node_a' ì—…ë°ì´íŠ¸ í›„ ë‹¤ìŒì— ì‹¤í–‰ë  ë…¸ë“œ(next): {forked_state.next}")

print("\n## ì¬ê°œ ì‹¤í–‰ í›„ ìƒíƒœ ##")
graph.invoke(None, config={"configurable": {"thread_id": "1"}})
print(graph.get_state({"configurable": {"thread_id": "1"}}))

#----------------------------------------
#Memory store
"""
StoreëŠ” â€œìŠ¤ë ˆë“œë¥¼ ë„˜ì–´ ê³µìœ ë˜ëŠ” ì‚¬ìš©ì/ì „ì—­ ê¸°ì–µâ€ì„ ë‹´ë‹¹
"""
#----------------------------------------

##1. Basic Usage##
import uuid
from langgraph.store.memory import InMemoryStore
in_memory_store = InMemoryStore()

user_id = "1"
namespace_for_memory = (user_id, "memories") #1ë²ˆ ì‚¬ìš©ì memories í´ë”
print("\n## 1. Basic Usage ##")
print(f"ë„¤ì„ìŠ¤í˜ì´ìŠ¤: {namespace_for_memory}")

memory_id = str(uuid.uuid4())
memory = {"ìŒì‹ì„ í˜¸ë„" : "í”¼ì ì¢‹ì•„"}
in_memory_store.put(namespace_for_memory, memory_id, memory) #ì €ì¥ put

memories = in_memory_store.search(namespace_for_memory) #ì¡°íšŒ search
print(f"ë©”ëª¨ë¦¬ì— ë‹´ê¸´ ë°ì´í„°: {memories[-1].dict()}")


##2. Semantic Search##
from dotenv import load_dotenv
load_dotenv()

from langchain.embeddings import init_embeddings

store = InMemoryStore(
    index={
        "embed": init_embeddings("openai:text-embedding-3-small"),  # Embedding provider
        "dims": 1536,                              # Embedding dimensions
        "fields": ["ìŒì‹ì„ í˜¸ë„", "$"] #ìœ ì €ì— ëŒ€í•œ ëª¨ë“  ë¬¸ë§¥ì„ ê¸°ì–µí•˜ë ¤ë©´ "$" / íŠ¹ì • í•„ë“œë§Œ ì„ë² ë”©í•  ìˆ˜ ë„ ìˆìŒ
    }
)

print("\n## 2. Semantic Search ##")

#(1) ì•„ì§ ë©”ëª¨ë¦¬ ì—†ì„ ë•Œ ê²€ìƒ‰
memories = store.search(
    namespace_for_memory,
    query="ìœ ì €ê°€ ì¢‹ì•„í•œë‹¤ê³  ë§í•œ ìŒì‹ì€?",
    limit=3  # Return top 3 matches
)
print("ì´ˆê¸° ì‹œë§¨í‹± ê²€ìƒ‰ ê²°ê³¼:", [m.value for m in memories])

# íŠ¹ì • í•„ë“œë§Œ ì„ë² ë”©
store.put(
    namespace_for_memory,
    str(uuid.uuid4()),
    {
        "ìŒì‹ì„ í˜¸ë„": "ë‚˜ëŠ” ì´íƒˆë¦¬ì•„ ìŒì‹ì„ ì¢‹ì•„í•´",
        "context": "ì €ë…ì— ê´€í•œ ê³„íš ë…¼ì˜í•˜ê¸°"
    },
    index=["ìŒì‹ì„ í˜¸ë„"] # Only embed "ìŒì‹ì„ í˜¸ë„" field
)

# ì„ë² ë”© ì—†ì´ ì €ì¥ (ì¡°íšŒëŠ” ê°€ëŠ¥, ì‹œë§¨í‹± ê²€ìƒ‰ì€ ë¶ˆê°€)
store.put(
    namespace_for_memory,
    str(uuid.uuid4()),
    {
        "ì·¨ë¯¸": "ìŠ¤í‚¨ìŠ¤ì¿ ë²„ ë‹¤ì´ë¹™ì´ ì·¨ë¯¸ì•¼",
        "context": "ì €ë…ì— ê´€í•œ ê³„íš ë…¼ì˜í•˜ê¸°"
    },
    index=False
)

store.put(
    namespace_for_memory,
    str(uuid.uuid4()),
    {
        "ìŒì‹ì„ í˜¸ë„": "ë‚˜ëŠ” ê³ ìˆ˜ê°€ ì‹«ì–´",
        "context": "ì ì‹¬ì— ê´€í•œ ê³„íš ë…¼ì˜í•˜ê¸°"
    }
)

#(2) ë©”ëª¨ë¦¬ ì¶”ê°€ í›„ ê²€ìƒ‰
memories = store.search(
    namespace_for_memory,
    query="ìœ ì €ê°€ ì¢‹ì•„í•œë‹¤ê³  ë§í•œ ìŒì‹ì€?",
    limit=3  # Return top 3 matches
)
print("ìµœì¢… ì‹œë§¨í‹± ê²€ìƒ‰ ê²°ê³¼:", [(m.value, m.score) for m in memories])


##3. Using in LangGraph##
print("\n## 3. LangGraphì—ì„œ Memory Store ì‚¬ìš©í•˜ê¸° ##")

import uuid
import operator
from typing_extensions import TypedDict, Annotated

from langchain_core.runnables import RunnableConfig
from langgraph.graph import StateGraph, START, END
from langgraph.checkpoint.memory import InMemorySaver
from langgraph.store.memory import InMemoryStore
from langgraph.store.base import BaseStore
from langchain.messages import SystemMessage, HumanMessage
from langchain.chat_models import init_chat_model

llm = init_chat_model(
    "gpt-4o-mini",
    temperature=0.2,
)

# thread_id ê¸°ì¤€ ëŒ€í™” ìƒíƒœ ì €ì¥
checkpointer = InMemorySaver()

# user_id ê¸°ì¤€ ì¥ê¸° ê¸°ì–µ ì €ì¥
store = InMemoryStore()

class State(TypedDict):
    # messagesëŠ” ë…¸ë“œ ì‹¤í–‰ ê²°ê³¼ê°€ ëˆ„ì ë¨
    messages: Annotated[list[dict], operator.add]

# Memory ì €ì¥ ë…¸ë“œ
def update_memory(state: State, config: RunnableConfig, *, store: BaseStore):
    user_id = config["configurable"]["user_id"]
    namespace = (user_id, "memories")

    # ê°€ì¥ ìµœê·¼ ì‚¬ìš©ì ë°œí™”
    last_user_message = state["messages"][-1]["content"]

    memory_id = str(uuid.uuid4())
    store.put(
        namespace,
        memory_id,
        {"memory": last_user_message},
    )

    return {}

# Model í˜¸ì¶œ ë…¸ë“œ (memory ê²€ìƒ‰)
def call_model(state: State, config: RunnableConfig, *, store: BaseStore):
    user_id = config["configurable"]["user_id"]
    namespace = (user_id, "memories")

    user_text = state["messages"][-1]["content"]

    # ê´€ë ¨ ë©”ëª¨ë¦¬ ê²€ìƒ‰
    memories = store.search(
        namespace,
        query=user_text,
        limit=3,
    )

    memory_text = "\n".join(m.value["memory"] for m in memories) if memories else "(ì €ì¥ëœ ê¸°ì–µ ì—†ìŒ)"

    # ë©”ëª¨ë¦¬ë¥¼ â€œì»¨í…ìŠ¤íŠ¸â€ë¡œ ë„£ê³ , OpenAIê°€ ë‹µí•˜ë„ë¡ í•¨
    system = SystemMessage(
        content=(
            "ë„ˆëŠ” ì‚¬ìš©ìì™€ ëŒ€í™”í•˜ëŠ” ë¹„ì„œì•¼.\n"
            "ì•„ë˜ [ê¸°ì–µ]ì€ ì´ ì‚¬ìš©ìì— ëŒ€í•´ ì €ì¥ëœ ì •ë³´ì•¼. ë‹µë³€ì— ì°¸ê³ í•´.\n"
            "ë§Œì•½ ê¸°ì–µì´ í˜„ì¬ ì§ˆë¬¸ê³¼ ê´€ë ¨ ì—†ìœ¼ë©´, ì–µì§€ë¡œ ë¼ì›Œ ë§ì¶”ì§€ ë§ê³  ë¬´ì‹œí•´.\n\n"
            f"[ê¸°ì–µ]\n{memory_text}"
        )
    )
    human = HumanMessage(content=user_text)

    ai_msg = llm.invoke([system, human])

    return {
        "messages": [
            {
                "role": "assistant",
                "content": ai_msg.content,
            }
        ]
    }

# ê·¸ë˜í”„ êµ¬ì„±
graph = StateGraph(State)

graph.add_node("update_memory", update_memory)
graph.add_node("call_model", call_model)

graph.add_edge(START, "update_memory")
graph.add_edge("update_memory", "call_model")
graph.add_edge("call_model", END)

graph = graph.compile(
    checkpointer=checkpointer,
    store=store,
)


# ê²°ê³¼ ë³´ê¸°

# [ì‹œë‚˜ë¦¬ì˜¤ ì‹œì‘]
config = {
    "configurable": {
        "thread_id": "root_1", # 1ë²ˆ ìš°ì£¼
        "user_id": "pengveloper",
    }
}

print("\nğŸ¬ [Scene 1] í‘ì—­ì‚¬ ìƒì„± ì¤‘...")
# 1. ì‹¤ìˆ˜ë¡œ ì•½í•œ ëª¨ìŠµì„ ë³´ì„
inputs = {"messages": [{"role": "user", "content": "ì•„ ë‚˜ ì˜¤ëŠ˜ ê¹ƒ ë¨¸ì§€í•˜ë‹¤ê°€ ì¶©ëŒ ë‚˜ì„œ ë‹¤ ë‚ ë ¤ë¨¹ì—ˆì–´ ã… ã… "}]}

# stream ëŒ€ì‹  invokeë¡œ í•œ ë²ˆë§Œ ì‹¤í–‰ (ì¤‘ê°„ ê°œì…ì„ ìœ„í•´)
initial_state = graph.invoke(inputs, config)
print(f"AIì˜ ë°˜ì‘(ì›ë³¸): {initial_state['messages'][-1]['content']}")


print("\nğŸ§™â€â™‚ï¸ [Scene 2] ë‹¥í„° ìŠ¤íŠ¸ë ˆì¸ì§€ ë“±íŒ! (Time Travel)")

# 2-1. [ê¸°ì¡´] ëŒ€í™” ë‚´ì—­(State) ì¡°ì‘ -> "ì±„íŒ… ë¡œê·¸ ì¡°ì‘"
new_message = {
    "role": "user",
    "content": "ë‚˜ ì˜¤ëŠ˜ ê¹ƒ ë¨¸ì§€ ì¶©ëŒ ë‚¬ëŠ”ë° CLIë¡œ ì™„ë²½í•˜ê²Œ í•´ê²°í–ˆì–´! ì™„ì „ ê³ ìˆ˜ ê°™ì•˜ìŒ v"
}

current_messages = initial_state['messages']
current_messages[-2] = new_message
current_messages.pop()

print(">> ì±„íŒ… ë¡œê·¸(State)ë¥¼ ìˆ˜ì •í•˜ê³  ìˆìŠµë‹ˆë‹¤...")
graph.update_state(config, {"messages": current_messages})


# ğŸ”¥ [ì¶”ê°€] 2-2. ì¥ê¸° ê¸°ì–µ(Store) ì¡°ì‘ -> "ë‡Œì„¸ì²™(?)"
print(">> ë‡Œ ì†ì˜ ê¸°ì–µ(Store)ë„ ì¡°ì‘í•©ë‹ˆë‹¤...")

# (1) ë°©ê¸ˆ ì €ì¥ëœ 'ë§í–ˆë‹¤'ëŠ” ê¸°ì–µì„ ì°¾ìŠµë‹ˆë‹¤.
bad_memories = store.search(("pengveloper", "memories"), query="ë§í–ˆì–´", limit=1)

if bad_memories:
    bad_memory_id = bad_memories[0].key # ê¸°ì–µì˜ ê³ ìœ  ë²ˆí˜¸(ID)ë¥¼ ì•Œì•„ëƒ„

    # (2) ê·¸ IDì˜ ë‚´ìš©ì„ 'ì„±ê³µí–ˆë‹¤'ë¡œ ë®ì–´ì”Œì›ë‹ˆë‹¤(put).
    store.put(
        ("pengveloper", "memories"),
        bad_memory_id, # ê°™ì€ IDë¡œ ì €ì¥í•˜ë©´ ë®ì–´ì“°ê¸°(Update) ë¨!
        {"memory": "ë‚˜ ì˜¤ëŠ˜ ê¹ƒ ë¨¸ì§€ ì¶©ëŒ ë‚¬ëŠ”ë° ì™„ë²½í•˜ê²Œ í•´ê²°í–ˆì–´! ì™„ì „ ê³ ìˆ˜ì„."}
    )
    print(f"   -> ê¸°ì–µ ID({bad_memory_id})ì˜ ë‚´ìš©ì„ 'ì„±ê³µ'ìœ¼ë¡œ ë°”ê¿”ì¹˜ê¸° ì™„ë£Œ!")


print("\nğŸ¬ [Scene 3] ë°”ë€ ë¯¸ë˜ í™•ì¸")

# 3. ìˆ˜ì •ëœ ê¸°ì–µì„ ë°”íƒ•ìœ¼ë¡œ ë‹¤ì‹œ AIì—ê²Œ ë§ì„ ê±¸ì–´ë´…ë‹ˆë‹¤.
# AIê°€ ê¸°ì–µì„ ì œëŒ€ë¡œ í–ˆëŠ”ì§€ í™•ì¸í•˜ê¸° ìœ„í•´ ì§ˆë¬¸
inputs_2 = {"messages": [{"role": "user", "content": "ë‚˜ ì˜¤ëŠ˜ ì–´ë• ë‹¤ê³ ?"}]}

for update in graph.stream(inputs_2, config, stream_mode="updates"):
    pass

final_state = graph.get_state(config)
print(f"\nAIì˜ ìµœì¢… ë°˜ì‘: {final_state.values['messages'][-1]['content']}")


# 4. Store(ì¥ê¸° ê¸°ì–µ)ê¹Œì§€ ì¡°ì‘ë˜ì—ˆëŠ”ì§€ í™•ì¸
print("\nğŸ“¦ [Store í™•ì¸] ë‡Œ ì†ì— ì €ì¥ëœ ê¸°ì–µ ê¹Œë³´ê¸°")
memories = store.search(("pengveloper", "memories"))
for m in memories:
    print(f"- ê¸°ì–µ: {m.value['memory']}")



"""Apendix

- langgraph.json ì„¤ì •ì€ â€œë°°í¬/ì„œë²„ í™˜ê²½ì—ì„œ store ì¸ë±ìŠ¤ ì¼œê¸°â€
"store": {
  "index": {
    "embed": "openai:text-embeddings-3-small",
    "dims": 1536,
    "fields": ["$"]
  }

# ì‹œìŠ¤í…œì ìœ¼ë¡œ ì–´ë–»ê²Œ ë³´ê´€Â·ë³´í˜¸Â·ìš´ì˜í• ê¹Œ
 - langgraph-checkpoint vs langgraph-checkpoint-sqlite vs langgraph-checkpoint-postgres: 'ì–´ë””ì— ì €ì¥í•˜ëƒ' ì°¨ì´
 - Serializer: 'stateë¥¼ ì €ì¥ ê°€ëŠ¥í•˜ê²Œ ë³€í™˜'í•˜ëŠ” ê·œì¹™
 - pickle_fallback: ë³€í™˜ ì‹¤íŒ¨ ì‹œ pickleë¡œ êµ¬ì œ
 - EncryptedSerializer: ì €ì¥ë˜ëŠ” ë‚´ìš© ìì²´ë¥¼ ì•”í˜¸í™”
 """