# ============================================================
# [Day 8] LangGraph í†µí•© ì˜ˆì œ: AI ëª…íƒì • ì…œë¡
#
# ì£¼ì œ: ì‚´ì¸ ì‚¬ê±´ ìˆ˜ì‚¬
# 1. Memory Store (ìˆ˜ì‚¬ ìˆ˜ì²©):
#    - ì¤‘ìš”í•œ ë‹¨ì„œëŠ” 'ìˆ˜ì²©'ì— ì˜êµ¬ ì €ì¥ë©ë‹ˆë‹¤.
#    - ë‹¤ë¥¸ í˜•ì‚¬(ìƒˆë¡œìš´ Thread)ê°€ ì™€ì„œ ë¬¼ì–´ë´ë„ ëŒ€ë‹µí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. (ì „ì—­ ê¸°ì–µ)
#
# 2. Checkpoints (ì·¨ì¡° ê¸°ë¡):
#    - ìš©ì˜ìì™€ì˜ ëŒ€í™” íë¦„ì„ ì €ì¥í•©ë‹ˆë‹¤.
#    - "ì ê¹, ì•„ê¹Œ ê·¸ ë§ ì·¨ì†Œí• ê²Œ"ë¼ë©° ê³¼ê±°ë¡œ ëŒì•„ê°€ ë‹¤ì‹œ ì‹¬ë¬¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. (íƒ€ì„ íŠ¸ë˜ë¸”)
# ============================================================

import os
import uuid
from dotenv import load_dotenv
from typing import Annotated, List
from typing_extensions import TypedDict
from operator import add

from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_core.messages import SystemMessage, HumanMessage, AIMessage, BaseMessage
from langchain_core.runnables import RunnableConfig

from langgraph.graph import StateGraph, START, END
from langgraph.checkpoint.memory import InMemorySaver
from langgraph.store.memory import InMemoryStore
from langgraph.store.base import BaseStore

# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
load_dotenv()

# ------------------------------------------------------------
# 1. ëª¨ë¸ ë° ì €ì¥ì†Œ ì„¤ì • (Gemini Free Tier)
# ------------------------------------------------------------

# LLM: ê°€ë³ê³  ë¹ ë¥¸ Flash ëª¨ë¸
llm = ChatGoogleGenerativeAI(model="gemini-2.5-flash", temperature=0)

# [ì¥ê¸° ê¸°ì–µ] ìˆ˜ì‚¬ ìˆ˜ì²©
# [ìˆ˜ì •] ì„ë² ë”© ëª¨ë¸ ì—†ì´ ê¸°ë³¸ ì €ì¥ì†Œë¡œ ì‚¬ìš© (API ì—ëŸ¬ ë°©ì§€)
memory_store = InMemoryStore()

# [ë‹¨ê¸° ê¸°ì–µ] ëŒ€í™” ìƒíƒœ ì €ì¥ì†Œ
checkpointer = InMemorySaver()


# ------------------------------------------------------------
# 2. ê·¸ë˜í”„ ìƒíƒœ(State) ë° ë…¸ë“œ ì •ì˜
# ------------------------------------------------------------

class DetectiveState(TypedDict):
    # ëŒ€í™” ë‚´ì—­ (ëˆ„ì ë¨)
    messages: Annotated[List[BaseMessage], add]

# ë…¸ë“œ 1: ì¶”ë¦¬ ë° ê¸°ë¡ (The Brain)
def detective_node(state: DetectiveState, config: RunnableConfig, *, store: BaseStore):
    user_id = config["configurable"].get("user_id", "default_user")
    namespace = (user_id, "case_file_001") # ì‚¬ê±´ ë²ˆí˜¸ 001ì— ëŒ€í•œ ìˆ˜ì²©

    # ì‚¬ìš©ìì˜ ë§ˆì§€ë§‰ ë§
    last_msg = state["messages"][-1].content

    # 1. [ê¸°ì–µ ê²€ìƒ‰] ìˆ˜ì²©ì—ì„œ ë‹¨ì„œ ì°¾ì•„ì˜¤ê¸°
    # [ìˆ˜ì •] query(ìœ ì‚¬ë„ ê²€ìƒ‰) ëŒ€ì‹  ë„¤ì„ìŠ¤í˜ì´ìŠ¤ì˜ ëª¨ë“  ê¸°ì–µì„ ê°€ì ¸ì˜µë‹ˆë‹¤.
    memories = store.search(namespace, limit=10)
    
    memory_context = ""
    if memories:
        # ê°€ì ¸ì˜¨ ë©”ëª¨ë¦¬ ê°ì²´ì—ì„œ ê°’(value) ì¶”ì¶œ
        found_clues = [m.value['content'] for m in memories]
        memory_context = "\n".join(found_clues)
        print(f"   ğŸ“– [ìˆ˜ì²© í™•ì¸] ê¸°ë¡ëœ ë‹¨ì„œë“¤: {found_clues}")
    else:
        print("   ğŸ“– [ìˆ˜ì²© í™•ì¸] ê¸°ë¡ëœ ë‹¨ì„œ ì—†ìŒ.")

    # 2. [LLM ì¶”ë¦¬] ë‹µë³€ ìƒì„±
    system_prompt = f"""
    ë‹¹ì‹ ì€ ëª…íƒì • ì…œë¡ì…ë‹ˆë‹¤. 
    ì‚¬ìš©ì(ë™ë£Œ í˜•ì‚¬ ë˜ëŠ” ì¦ì¸)ì™€ ëŒ€í™”í•˜ë©° ì‚¬ê±´ì„ ìˆ˜ì‚¬í•˜ì„¸ìš”.
    
    [ìˆ˜ì‚¬ ìˆ˜ì²©(ì¥ê¸° ê¸°ì–µ)]
    {memory_context}
    
    ì§€ì‹œì‚¬í•­:
    1. ì‚¬ìš©ìê°€ ìƒˆë¡œìš´ ë‹¨ì„œ(ë²”ì¸ íŠ¹ì§•, ì¥ì†Œ ë“±)ë¥¼ ë§í•˜ë©´ "ë‹¨ì„œê°€ ê¸°ë¡ë˜ì—ˆìŠµë‹ˆë‹¤"ë¼ê³  ë§í•˜ì„¸ìš”.
    2. ìˆ˜ì²©ì˜ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ ì¶”ë¦¬í•˜ì—¬ ë‹µë³€í•˜ì„¸ìš”.
    3. ê±°ë§Œí•˜ì§€ë§Œ ì²œì¬ì ì¸ ë§íˆ¬ë¥¼ ì‚¬ìš©í•˜ì„¸ìš”.
    """

    response = llm.invoke([
        SystemMessage(content=system_prompt),
        *state["messages"] # ëŒ€í™” íˆìŠ¤í† ë¦¬ ì „ì²´ ì „ë‹¬
    ])

    # 3. [ê¸°ì–µ ì €ì¥] ë§Œì•½ ì¤‘ìš”í•œ ë‹¨ì„œë¼ë©´ ìˆ˜ì²©ì— ê¸°ë¡
    if "ë‹¨ì„œ" in last_msg or "ë²”ì¸" in last_msg or "ì¦ê±°" in last_msg or "ìŠ¤ì¹´í”„" in last_msg or "ë„¥íƒ€ì´" in last_msg:
        print(f"   âœï¸ [ìˆ˜ì²© ê¸°ë¡] ìƒˆë¡œìš´ ì •ë³´ë¥¼ ì ìŠµë‹ˆë‹¤...")
        store.put(
            namespace,
            str(uuid.uuid4()),
            {"content": last_msg}
        )

    return {"messages": [response]}


# ------------------------------------------------------------
# 3. ê·¸ë˜í”„ ë¹Œë“œ
# ------------------------------------------------------------
workflow = StateGraph(DetectiveState)
workflow.add_node("detective", detective_node)
workflow.add_edge(START, "detective")
workflow.add_edge("detective", END)

# ì²´í¬í¬ì¸í„°(ëŒ€í™”ìš©)ì™€ ìŠ¤í† ì–´(ìˆ˜ì²©ìš©)ë¥¼ ëª¨ë‘ ì¥ì°©
app = workflow.compile(checkpointer=checkpointer, store=memory_store)


# ============================================================
# 4. ì‹¤í–‰ ì‹œë‚˜ë¦¬ì˜¤ (Simulation)
# ============================================================

# --- [Scene 1] ì™“ìŠ¨ê³¼ì˜ ëŒ€í™” (ë‹¨ì„œ ìˆ˜ì§‘) ---
config_watson = {
    "configurable": {
        "thread_id": "watson_session",
        "user_id": "scotland_yard" # ìˆ˜ì²© ê³µìœ ë¥¼ ìœ„í•œ ID
    }
}

print("\nğŸ•µï¸ [Scene 1] ì™“ìŠ¨ ë°•ì‚¬ê°€ í˜„ì¥ ì •ë³´ë¥¼ ë³´ê³ í•©ë‹ˆë‹¤.")
print("-" * 50)

input_1 = {"messages": [HumanMessage(content="ì…œë¡, ë‹¨ì„œê°€ ë‚˜ì™”ì–´. ë²”ì¸ì€ 'ì™¼ìª½ ë‹¤ë¦¬ë¥¼ ì „ë‹¤'ê³  í•´.")]}
for update in app.stream(input_1, config_watson, stream_mode="updates"):
    print(f"ì…œë¡: {update['detective']['messages'][0].content}")

input_2 = {"messages": [HumanMessage(content="ê·¸ë¦¬ê³  í˜„ì¥ì—ì„œ 'ë¹¨ê°„ìƒ‰ ìŠ¤ì¹´í”„'ê°€ ë°œê²¬ëì–´.")]}
for update in app.stream(input_2, config_watson, stream_mode="updates"):
    print(f"ì…œë¡: {update['detective']['messages'][0].content}")


# --- [Scene 2] ë ˆìŠ¤íŠ¸ë ˆì´ë“œ ê²½ê°ê³¼ì˜ ëŒ€í™” (ê¸°ì–µ ê³µìœ  í™•ì¸) ---
print("\nğŸ‘® [Scene 2] ë ˆìŠ¤íŠ¸ë ˆì´ë“œ ê²½ê°ì´ ìˆ˜ì‚¬ ìƒí™©ì„ ë¬»ìŠµë‹ˆë‹¤. (ë‹¤ë¥¸ ì“°ë ˆë“œ)")
print("-" * 50)

config_lestrade = {
    "configurable": {
        "thread_id": "lestrade_session",
        "user_id": "scotland_yard" # ì™“ìŠ¨ê³¼ ê°™ì€ ìˆ˜ì²©ì„ ê³µìœ 
    }
}

input_3 = {"messages": [HumanMessage(content="ì–´ì´ ì…œë¡, ë²”ì¸ì˜ ì¸ìƒì°©ì˜ì— ëŒ€í•´ ì•Œì•„ë‚¸ ê²Œ ìˆë‚˜?")]}
for update in app.stream(input_3, config_lestrade, stream_mode="updates"):
    print(f"ì…œë¡(To ê²½ê°): {update['detective']['messages'][0].content}")


# --- [Scene 3] íƒ€ì„ íŠ¸ë˜ë¸” (Checkpoints í™œìš©) ---
print("\nâ³ [Scene 3] íƒ€ì„ íŠ¸ë˜ë¸”: ì™“ìŠ¨ê³¼ì˜ ëŒ€í™” ì¤‘ 'ë¹¨ê°„ ìŠ¤ì¹´í”„' ì–˜ê¸° ì „ìœ¼ë¡œ ë˜ê°ê¸°")
print("-" * 50)

history = list(app.get_state_history(config_watson))
target_checkpoint = history[1].config
print(f"ëŒì•„ê°ˆ ì‹œì  ID: {target_checkpoint['configurable']['checkpoint_id']}")

past_state = app.get_state(target_checkpoint)
print(f"ê³¼ê±° ì‹œì ì˜ ëŒ€í™” ë‚´ìš©: {[m.content for m in past_state.values['messages']]}")

print("\nâ–¶ï¸ [ì¬ê°œ] ê³¼ê±° ì‹œì ì—ì„œ ë‹¤ì‹œ ëŒ€í™”í•©ë‹ˆë‹¤. (ë¹¨ê°„ ìŠ¤ì¹´í”„ ëŒ€ì‹  ë‹¤ë¥¸ ì •ë³´ ì…ë ¥)")
config_forked = target_checkpoint

input_fork = {"messages": [HumanMessage(content="ì•„ ì •ì •í• ê²Œ. ìŠ¤ì¹´í”„ê°€ ì•„ë‹ˆë¼ 'íŒŒë€ìƒ‰ ë„¥íƒ€ì´'ì˜€ì–´.")]}
for update in app.stream(input_fork, config_forked, stream_mode="updates"):
    print(f"ì…œë¡(ê³¼ê±° ìˆ˜ì •): {update['detective']['messages'][0].content}")
