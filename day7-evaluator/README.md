# 오늘의 서기: 선태영
# day7-evaluator

## 오늘의 핵심
- Evaluator[평가자] : LangGraph 흐름 내에서 Agent 또는 LLM 노드의 출력 결과를 기준(정답 여부, 응답 품질, 조건 충족 여부 등)에 따라 평가하고, 그 결과를 다음 분기·재시도·종료 판단에 활용하는 노드 또는 로직
-

## 회고 모음
- {{이영근}}: llm의 결과를 평가하고 개선하게 자동화
- {{임정수}}: 유명인들과 인맥이 생겼습니다.
- {{양승준}}: 평가-개선 루프 과정을 만드는법을 배웠다.
- {{홍자영}}: 맨날 장난처럼 얘기하던 ai(llm)들한테 지금부터 너희끼리 싸워라!를 이런 식으로도 할 수 있겠구나 싶어서 재미있었고, 스스로 평가까지 해주니 편리함!!
- {{박성빈}}: AI 통제관을 붙혀 답변 품질을 향상시킨다, 안되면 될때까지
- {{이선영}}: Generator가 답을 만들고 → Evaluator가 합격/불합격을 판정 → 불합격이면 피드백을 주고 Generator에서 다시 생성한다. 이 때 무한 루프를 돌지 않기 위한 횟수 제한은 llm_call_generator에서 한다.
- {{임은상}}: 생성해서 평가하며 필요하면 피드백 반영해서 재생성하는 평가 개선 루프
- {{윤지연}}: 지금부터 서로 죽여라
- {{김동국}}: AI가 스스로 검토하고 고치면서 더 나은 결과를 만들어낸다?
- {{선태영}}: 내가 생각한건 천재들이 이미 다 만들었다.

## 자주 나온 질문
- Q.매번 무료 API 키를 토큰 제한 때문에 바꿔야 돼서 힘듭니다.
- Q.정말 끔찍한 유머감각이네요. 단순히 피드백을 보는것말고 llm의 유머를 판단하는 사고과정과 기준을 알 수도 있을까요?
    - A.reasoning 모델을 써볼수도 있을거 같구요. prompt에서 chain of thought를 적용해볼 수 있을거 같아요. 설명 가능한 판정 기준을 세워서 로그를 보는 편이 가장 확실할 것 같다는 생각이 드네요
- Q.가끔 다른 분들 PR을 보면 무슨 약을 하시길래 저런 창의적인 생각을?!하는 생각이 들었는데 저도 오늘은 유머러스하게 해보고 싶었어요:9
- Q.조건을 너무 빡빡하게 붙히면 대답이 안나오기도 할까요?
    - A.대답이 안나온다는게 뭘까요? 아무것도 안나오는 일은 없을거구 Literal에 맞지 않은 값이 나온다거나 하면 에러 발생할 것 같네요
- Q.joke이기만 하면 무조건 funny로 판정되는 건 아닐까?라는 의문이 들었고,
  검색해보니 평가 기준을 프롬프트로 명확히 주지 않으면
  funny 판정 쪽으로 편향될 수 있다고 이해했는데 맞는지..??
    - A.그럴수도 있을거 같아요. 어쨌든 joke를 기준으로 문장을 생성했으니까 보편적인 llm 기준이라면 편향될 거 같아요.
      그래서 다른 모델로 평가한다던가, 아니면 프롬프트에 자세한 기준을 넣어서 평가하면 좋을것 같네요
- Q.llm끼리 토론하고 개선하게 하는 원리는 뭘까요?
    - A.음..원리라 우선 LLM을 평가할 때 LLM 기반 평가는 많이 쓰이는 방법입니다. llm이 출력한 결과값이 얼마나 괜찮은지 평가하는거죠. 보통 이럴때는 다른 모델을 쓰곤합니다