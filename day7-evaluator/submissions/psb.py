#ê¸°ë³¸ ì„¤ì •
import os
from dotenv import load_dotenv
load_dotenv()

#-------------------------------------
#ëª¨ë¸ ì„¤ì •
#-------------------------------------
from langchain_google_genai import ChatGoogleGenerativeAI 
llm = ChatGoogleGenerativeAI(model="gemini-2.0-flash")

from typing_extensions import TypedDict
from langgraph.graph import StateGraph, START, END
from pydantic import BaseModel, Field
from typing import Literal #ì£¼ì–´ì§„ ë³´ê¸° ì•ˆì—ì„œ ì„ íƒí•˜ë„ë¡ ê°•ì œ

# DB ì„¤ì •
from pymongo import MongoClient
MongoDB_URI = os.getenv("MONGODB_URI")
client = MongoClient(MongoDB_URI)
DB = client["brickers"]
collection = DB["ldraw_parts"]

#-------------------------------------
#Evaluator-optimizer(í‰ê°€-ê°œì„  ë£¨í”„) *ì˜ˆì œì™€ ë‹¤ë¥´ê²Œ ê°œì„  ë£¨í”„ 3íšŒ ì œí•œ
#-------------------------------------
# Graph state

class State(TypedDict):
    target_part: str      # ì‚¬ìš©ìê°€ ìš”ì²­í•œ ë¶€í’ˆëª… (ì˜ˆ: "Brick 2x4")
    db_data: str          # DBì—ì„œ ì°¾ì•„ë‚¸ ë¶€í’ˆ ì •ë³´ (Context)
    description: str      # AIê°€ ì‘ì„±í•œ ì„¤ëª…ê¸€ (Draft)
    feedback: str         # í‰ê°€ìì˜ í”¼ë“œë°±
    grade: str            # í‰ê°€ ê²°ê³¼ (pass / rewrite)
    attempts: int         # ì‹œë„ íšŸìˆ˜

# í‰ê°€ ê²°ê³¼ë¥¼ êµ¬ì¡°í™”í•  ìŠ¤í‚¤ë§ˆ(Feedback)
class Feedback(BaseModel):
    # evaluatorì˜ ì¶œë ¥ í˜•ì‹ì„ ê°•ì œë¡œ ê³ ì •
    grade: Literal["pass", "rewrite"] = Field(
        description="ì„¤ëª…ê¸€ì´ ì™„ë²½í•˜ë©´ 'pass', ìˆ˜ì •ì´ í•„ìš”í•˜ë©´ 'rewrite'ë¥¼ ì„ íƒí•˜ì„¸ìš”."
    )
    feedback: str = Field(
        description="ìˆ˜ì •ì´ í•„ìš”í•˜ë‹¤ë©´ êµ¬ì²´ì ìœ¼ë¡œ ì–´ë–¤ ì •ë³´(ID, ìƒ‰ìƒ ë“±)ê°€ ëˆ„ë½ë˜ì—ˆëŠ”ì§€, í˜¹ì€ í†¤ì•¤ë§¤ë„ˆë¥¼ ì–´ë–»ê²Œ ê³ ì³ì•¼ í• ì§€ ì¡°ì–¸í•˜ì„¸ìš”."
    )


# evaluator(í‰ê°€ LLM) ë§Œë“¤ê¸°
evaluator = llm.with_structured_output(Feedback)


# ë…¸ë“œ 1: llm_call_generator (ìƒì„±ê¸°)
def llm_call_generator(state: State):
    """DB ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì„¤ëª…ê¸€ì„ ì‘ì„±í•˜ê±°ë‚˜, í”¼ë“œë°±ì„ ë°˜ì˜í•´ ìˆ˜ì •í•©ë‹ˆë‹¤."""

    # ì‹œë„ íšŸìˆ˜ ì¹´ìš´íŠ¸ (ì—†ìœ¼ë©´ 0ìœ¼ë¡œ ì‹œì‘)
    current_attempts = state.get("attempts") or 0
    new_attempts = current_attempts + 1

    # ì²« ì‹œë„ì¼ ë•Œë§Œ DB ê²€ìƒ‰ ìˆ˜í–‰ (Context í™•ë³´)
    db_context = state.get("db_data")
    if not db_context:
        print(f"ğŸ” [SEARCH] '{state['target_part']}' DB ê²€ìƒ‰ ì¤‘...")
        search_res = list(collection.find(
            {
                "$or": [
                    {"name": {"$regex": state['target_part'], "$options": "i"}}, # íŒŒì¼ ì´ë¦„
                    {"keywords": {"$regex": state['target_part'], "$options": "i"}}, # í‚¤ì›Œë“œ
                    {"partId": {"$regex": state['target_part'], "$options": "i"}} # ë¶€í’ˆ ID
                ]
            },
        {"_id": 0, "name": 1, "partId": 1, "keywords": 1, "category": 1}
        ).limit(1))
        
        if search_res:
            db_context = str(search_res[0])
        else:
            db_context = "DBì— í•´ë‹¹ ë¶€í’ˆ ì •ë³´ê°€ ì—†ìŠµë‹ˆë‹¤."

    # í”„ë¡¬í”„íŠ¸ êµ¬ì„±
    if state.get("feedback"):
        # ì¬ì‹œë„: í”¼ë“œë°± ë°˜ì˜
        prompt = f"""
        ë‹¹ì‹ ì€ ë ˆê³  ë§ˆì¼€íŒ… ì „ë¬¸ê°€ì…ë‹ˆë‹¤. 
        ì•„ë˜ [ê¸°ì¡´ ì´ˆì•ˆ]ì„ [í”¼ë“œë°±]ì— ë§ì¶°ì„œ í›¨ì”¬ ë” ë§¤ë ¥ì ì´ê³  ì •í™•í•˜ê²Œ ë‹¤ì‹œ ì‘ì„±í•˜ì„¸ìš”.
        
        [ë¶€í’ˆ ì •ë³´]: {db_context}
        [ê¸°ì¡´ ì´ˆì•ˆ]: {state['description']}
        [í”¼ë“œë°±]: {state['feedback']}
        """
    else:
        # ì²« ì‹œë„: ì‹ ê·œ ì‘ì„±
        prompt = f"""
        ë‹¹ì‹ ì€ ë ˆê³  ë§ˆì¼€íŒ… ì „ë¬¸ê°€ì…ë‹ˆë‹¤. 
        ì œê³µëœ [ë¶€í’ˆ ì •ë³´]ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì‡¼í•‘ëª°ì— ì˜¬ë¦´ ìƒì„¸í•˜ê³  ë§¤ë ¥ì ì¸ ìƒí’ˆ ì†Œê°œê¸€ì„ ì‘ì„±í•˜ì„¸ìš”.
        ë°˜ë“œì‹œ ë¶€í’ˆì˜ IDì™€ ì´ë¦„ì„ ì •í™•í•˜ê²Œ ëª…ì‹œí•´ì•¼ í•©ë‹ˆë‹¤.
        
        [ë¶€í’ˆ ì •ë³´]: {db_context}
        """

    msg = llm.invoke(prompt)
    
    print(f"\nğŸ“ [DRAFT {new_attempts}] ìƒì„±ë¨:\n{msg.content[:100]}...") # ì•ë¶€ë¶„ë§Œ ì‚´ì§ ì¶œë ¥

    return {
        "description": msg.content, 
        "attempts": new_attempts, 
        "db_data": db_context # DB ì •ë³´ ì €ì¥í•´ë‘ê¸°
    }


# ë…¸ë“œ 2: llm_call_evaluator (í‰ê°€ì)
def llm_call_evaluator(state: State):
    """ì‘ì„±ëœ ì„¤ëª…ê¸€ì„ ê¹ê¹í•˜ê²Œ í‰ê°€í•©ë‹ˆë‹¤."""

    prompt = f"""
    ë‹¹ì‹ ì€ ì„¸ìƒì—ì„œ ê°€ì¥ ì„±ê²©ì´ ê¼¬ì¸ ì•…ë• í¸ì§‘ì¥ì…ë‹ˆë‹¤.
    ì•„ë˜ [ì„¤ëª…ê¸€]ì´ ë‹¤ìŒ 'ê¸°ì¤€'ì„ ëª¨ë‘ ë§Œì¡±í•˜ì§€ ëª»í•˜ë©´ ê°€ì°¨ ì—†ì´ 'rewrite'ë¥¼ ì£¼ê³  ë…ì„¤ì„ í¼ë¶€ìœ¼ì„¸ìš”.
    
    [í‰ê°€ ê¸°ì¤€ - í•˜ë‚˜ë¼ë„ í‹€ë¦¬ë©´ íƒˆë½]
    1. [ì´ëª¨ì§€ í­íƒ„]: ë³¸ë¬¸ì— ì´ëª¨ì§€ê°€ **ì •í™•íˆ 7ê°œ** í¬í•¨ë˜ì–´ì•¼ í•¨. (ë¬´ì¡°ê±´ 7ê°œ)
    2. [ì˜¤ê¸€ê±°ë¦¼]: ë¬¸ì¥ì˜ ì‹œì‘ì€ ë¬´ì¡°ê±´ "ì£¼ëª©í•˜ë¼, ë ˆê³  ë•í›„ë“¤ì´ì—¬!" ë¡œ ì‹œì‘í•´ì•¼ í•¨.
    3. [íŠ¹ì • ë‹¨ì–´]: ë³¸ë¬¸ì— "ì§€ê°‘ í„¸ë¦´ ì¤€ë¹„ ë˜ì…¨ë‚˜ìš”?" ë¼ëŠ” ë¬¸êµ¬ê°€ ë°˜ë“œì‹œ í¬í•¨ë˜ì–´ì•¼ í•¨.
    4. [í˜•ì‹]: ë§ˆì§€ë§‰ ì¤„ì€ ë°˜ë“œì‹œ í•´ì‹œíƒœê·¸ 3ê°œ(#ë ˆê³  #ë¸Œë¦­ #í•„ìˆ˜í…œ)ë¡œ ëë‚˜ì•¼ í•¨.
    
    [DB ì •ë³´]: {state['db_data']}
    [ì„¤ëª…ê¸€]: {state['description']}
    """
    
    result = evaluator.invoke(prompt)
    
    print(f"[EVAL {state['attempts']}] í¸ì§‘ì¥ íŒì •: {result.grade.upper()}")
    if result.grade == "rewrite":
        print(f"   ğŸ”¥ ë…ì„¤ í”¼ë“œë°±: {result.feedback}")

    return {"grade": result.grade, "feedback": result.feedback}

# ë¼ìš°íŒ… í•¨ìˆ˜
def route_decision(state: State):
    if state["grade"] == "pass":
        print("[SUCCESS] í†µê³¼! ì™„ë£Œí•©ë‹ˆë‹¤.")
        return "Accepted"
    
    if state["attempts"] >= 3:
        print("[STOP] 3íšŒ ì‹œë„ ì´ˆê³¼. ê°•ì œ ì¢…ë£Œí•©ë‹ˆë‹¤.")
        return "Accepted"
    
    print("[LOOP] ë‹¤ì‹œ ì‘ì„±í•˜ëŸ¬ ê°‘ë‹ˆë‹¤...")
    return "Retry"


# Build workflow
optimizer_builder = StateGraph(State)

# Add the nodes
optimizer_builder.add_node("llm_call_generator", llm_call_generator)
optimizer_builder.add_node("llm_call_evaluator", llm_call_evaluator)

# Add edges to connect nodes
optimizer_builder.add_edge(START, "llm_call_generator")
optimizer_builder.add_edge("llm_call_generator", "llm_call_evaluator")
optimizer_builder.add_conditional_edges(
    "llm_call_evaluator",
    route_decision,
    {  # Name returned by route_decision : Name of next node to visit
        "Accepted": END,
        "Retry": "llm_call_generator",
    },
)

# Compile the workflow
optimizer_workflow = optimizer_builder.compile()

# Show the workflow
print("Here is the mermaid graph syntax. You can paste it into https://mermaid.live/ :") #ì‚¬ì´íŠ¸ ë“¤ì–´ê°€ì„œ ì½”ë“œ ë¶™ì—¬ë„£ê¸°
print(optimizer_workflow.get_graph(xray=True).draw_mermaid())

# Invoke
print("-" * 50)
# ì˜ˆ: 'Brick' ê²€ìƒ‰í•´ì„œ ë§ˆì¼€íŒ… ë¬¸êµ¬ ì¨ì¤˜
result = optimizer_workflow.invoke({"target_part": "Brick"}) 

print("\n" + "="*50)
print("[ìµœì¢… ê²°ê³¼ë¬¼]")
print(result["description"])
print("="*50)