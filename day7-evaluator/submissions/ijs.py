#ê¸°ë³¸ ì„¤ì •
import os
from dotenv import load_dotenv
load_dotenv()

#-------------------------------------
#ëª¨ë¸ ì„¤ì •
#-------------------------------------
from langchain_google_genai import ChatGoogleGenerativeAI
llm = ChatGoogleGenerativeAI(model="gemini-2.5-flash")

from typing_extensions import TypedDict
from langgraph.graph import StateGraph, START, END
from pydantic import BaseModel, Field
from typing import Literal, List #ì£¼ì–´ì§„ ë³´ê¸° ì•ˆì—ì„œ ì„ íƒí•˜ë„ë¡ ê°•ì œ

#-------------------------------------
#Evaluator-optimizer(í‰ê°€-ê°œì„  ë£¨í”„) *ì˜ˆì œì™€ ë‹¤ë¥´ê²Œ ê°œì„  ë£¨í”„ 3íšŒ ì œí•œ
#ì‹¬ì‚¬ìœ„ì› 3ëª…ì´ ê°ê° 0~100ì  í‰ê°€, ì´í•© 250ì  ì´ìƒì´ë©´ í•©ê²©
#-------------------------------------
# Graph state

class State(TypedDict):
    joke: str
    topic: str
    feedback: str
    scores: List[int]  # 3ëª…ì˜ ì‹¬ì‚¬ìœ„ì› ì ìˆ˜
    total_score: int   # ì´ì 
    pass_or_fail: str  # í•©ê²©/ë¶ˆí•©ê²©
    attempts: int  # ê°œì„  ì‹œë„ íšŸìˆ˜ ì¹´ìš´í„°


# í‰ê°€ ê²°ê³¼ë¥¼ êµ¬ì¡°í™”í•  ìŠ¤í‚¤ë§ˆ(JudgeScore) - ê° ì‹¬ì‚¬ìœ„ì›ìš©
class JudgeScore(BaseModel):
    score: int = Field(
        description="Score the joke from 0 to 100 based on how funny it is.",
        ge=0,
        le=100
    )
    feedback: str = Field(
        description="Provide feedback on the joke and how to improve it.",
    )
    review: str = Field(
        description="Write a detailed review/commentary about the joke in Korean as a judge would say on a TV show.",
    )


# 3ëª…ì˜ ì‹¬ì‚¬ìœ„ì› evaluator ìƒì„± (ê°ê° ë‹¤ë¥¸ ê´€ì )
judge1 = llm.with_structured_output(JudgeScore)
judge2 = llm.with_structured_output(JudgeScore)
judge3 = llm.with_structured_output(JudgeScore)


# ë…¸ë“œ 1: llm_call_generator (ìƒì„±ê¸°)
def llm_call_generator(state: State):
    """LLM generates a joke"""

    # ì‹œë„ íšŸìˆ˜ ì¹´ìš´íŠ¸ (ì—†ìœ¼ë©´ 0ìœ¼ë¡œ ì‹œì‘)
    current_attempts = state.get("attempts") or 0
    new_attempts = current_attempts + 1

    if state.get("feedback"):
        msg = llm.invoke(
            f"Write a joke about {state['topic']} but take into account the feedback: {state['feedback']}"
        )
    else:
        msg = llm.invoke(f"Write a joke about {state['topic']}")
    
    # ì–´ë–¤ ë‚´ìš©ì„ ìƒì„±í–ˆëŠ”ì§€ ì¶œë ¥
    print(f"[GENERATE] Joke {new_attempts}: {msg.content[:100]}\n")

    return {"joke": msg.content, "attempts": new_attempts}


# ë…¸ë“œ 2: llm_call_evaluator (3ëª…ì˜ ì‹¬ì‚¬ìœ„ì›ì´ í‰ê°€)
def llm_call_evaluator(state: State):
    """3 LLM judges evaluate the joke"""

    joke = state['joke']

    # ì‹¬ì‚¬ìœ„ì› 1: ì•ˆì„±ì¬ - ìœ ë¨¸ ê°ê° ê¸°ì¤€
    result1 = judge1.invoke(
        f"You are ì•ˆì„±ì¬ (Ahn Sung-jae), a famous Korean chef judge known for your sharp wit and high standards. Focus on cleverness and wit. Score this joke from 0 to 100 and provide a detailed review in Korean as you would on a TV show: {joke}"
    )

    # ì‹¬ì‚¬ìœ„ì› 2: ê°•ë ˆì˜¤ - ì°½ì˜ì„± ê¸°ì¤€
    result2 = judge2.invoke(
        f"You are ê°•ë ˆì˜¤ (Chef Leo Kang), a passionate and expressive Korean chef judge. Focus on creativity and originality. Score this joke from 0 to 100 and provide a detailed review in Korean as you would on a TV show: {joke}"
    )

    # ì‹¬ì‚¬ìœ„ì› 3: ì—ë“œì›Œë“œ ë¦¬ - ì „ë‹¬ë ¥ ê¸°ì¤€
    result3 = judge3.invoke(
        f"You are ì—ë“œì›Œë“œ ë¦¬ (Edward Lee), a Korean-American chef judge known for your warm and thoughtful critiques. Focus on delivery and timing. Score this joke from 0 to 100 and provide a detailed review in Korean as you would on a TV show: {joke}"
    )

    scores = [result1.score, result2.score, result3.score]
    total_score = sum(scores)

    # í•©ê²©/ë¶ˆí•©ê²© íŒì •
    pass_or_fail = "pass" if total_score >= 250 else "fail"

    # í”¼ë“œë°± í†µí•©
    combined_feedback = f"""
    [ì•ˆì„±ì¬ (ìœ ë¨¸ê°ê°)] ì ìˆ˜: {result1.score}/100 - {result1.feedback}
    [ê°•ë ˆì˜¤ (ì°½ì˜ì„±)] ì ìˆ˜: {result2.score}/100 - {result2.feedback}
    [ì—ë“œì›Œë“œ ë¦¬ (ì „ë‹¬ë ¥)] ì ìˆ˜: {result3.score}/100 - {result3.feedback}
    """

    # í‰ê°€ ê²°ê³¼ ì¶œë ¥
    print(f"[EVALUATE] Attempt {state['attempts']}:")
    print(f"\n  ğŸ‘¨â€ğŸ³ ì•ˆì„±ì¬ ì‹¬ì‚¬ìœ„ì›: {result1.score}ì ")
    print(f"     ì‹¬ì‚¬í‰: {result1.review}")
    print(f"\n  ğŸ‘¨â€ğŸ³ ê°•ë ˆì˜¤ ì‹¬ì‚¬ìœ„ì›: {result2.score}ì ")
    print(f"     ì‹¬ì‚¬í‰: {result2.review}")
    print(f"\n  ğŸ‘¨â€ğŸ³ ì—ë“œì›Œë“œ ë¦¬ ì‹¬ì‚¬ìœ„ì›: {result3.score}ì ")
    print(f"     ì‹¬ì‚¬í‰: {result3.review}")
    print(f"\n  ğŸ“Š ì´ì : {total_score}/300 ({'í•©ê²©' if pass_or_fail == 'pass' else 'ë¶ˆí•©ê²©'})\n")

    return {
        "scores": scores,
        "total_score": total_score,
        "pass_or_fail": pass_or_fail,
        "feedback": combined_feedback
    }


# ë¼ìš°íŒ… í•¨ìˆ˜
def route_joke(state: State):
    """Route back to joke generator or end based upon feedback from the evaluators"""

    if state["pass_or_fail"] == "pass":
        print(f"[END] í•©ê²©! ì´ì  {state['total_score']}/300 (250ì  ì´ìƒ)")
        return "Accepted"
    elif state["pass_or_fail"] == "fail":
        # 3íšŒ ì´ìƒ ì‹œë„í–ˆë‹¤ë©´ ì—¬ê¸°ì„œ ë©ˆì¶¤ (Accepted ë¦¬í„´ -> END)
        if state.get("attempts", 0) >= 3:
            print(f"[END] 3íšŒ ê°œì„  ì‹œë„ ë„ë‹¬ â†’ ê°•ì œ ì¢…ë£Œ (ìµœì¢… ì ìˆ˜: {state['total_score']}/300)")
            return "Accepted"

        print(f"[LOOP] ë¶ˆí•©ê²© (ì´ì : {state['total_score']}/300) â†’ í”¼ë“œë°± ë°˜ì˜í•˜ì—¬ ì¬ì‹œë„")
        return "Rejected + Feedback"


# Build workflow
optimizer_builder = StateGraph(State)

# Add the nodes
optimizer_builder.add_node("llm_call_generator", llm_call_generator)
optimizer_builder.add_node("llm_call_evaluator", llm_call_evaluator)

# Add edges to connect nodes
optimizer_builder.add_edge(START, "llm_call_generator")
optimizer_builder.add_edge("llm_call_generator", "llm_call_evaluator")
optimizer_builder.add_conditional_edges(
    "llm_call_evaluator",
    route_joke,
    {  # Name returned by route_joke : Name of next node to visit
        "Accepted": END,
        "Rejected + Feedback": "llm_call_generator",
    },
)

# Compile the workflow
optimizer_workflow = optimizer_builder.compile()

# Show the workflow
print("Here is the mermaid graph syntax. You can paste it into https://mermaid.live/ :") #ì‚¬ì´íŠ¸ ë“¤ì–´ê°€ì„œ ì½”ë“œ ë¶™ì—¬ë„£ê¸°
print(optimizer_workflow.get_graph(xray=True).draw_mermaid())

# Invoke
state = optimizer_workflow.invoke({"topic": "Cats"})
print("\n" + "="*50)
print("ìµœì¢… ë†ë‹´:")
print(state["joke"])
print(f"\nìµœì¢… ì ìˆ˜: {state.get('total_score', 0)}/300")
print(f"ì‹¬ì‚¬ìœ„ì›ë³„ ì ìˆ˜: {state.get('scores', [])}")
